{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    \n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def gen_features(X):\n",
    "    strain = []\n",
    "    strain.append(X.mean())\n",
    "    strain.append(X.std())\n",
    "    strain.append(X.min())\n",
    "    strain.append(X.max())\n",
    "    strain.append(X.kurtosis())\n",
    "    strain.append(X.skew())\n",
    "    strain.append(np.quantile(X,0.01))\n",
    "    strain.append(np.quantile(X,0.05))\n",
    "    strain.append(np.quantile(X,0.95))\n",
    "    strain.append(np.quantile(X,0.99))\n",
    "    strain.append(np.abs(X).max())\n",
    "    strain.append(np.abs(X).mean())\n",
    "    strain.append(np.abs(X).std())\n",
    "    return pd.Series(strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy shape :  (41943, 4) y shape :  (41943, 1)\n",
      "train_setx shape :  (29360, 4) train_sety shape :  (29360, 1)\n",
      "test_setx shape :  (12593, 4) test_sety shape :  (12593, 1)\n",
      "trainX shape :  (29350, 10, 4) trainY shape :  (29350, 1)\n",
      "testX shape :  (12583, 10, 4) testY shape :  (12583, 1)\n",
      "WARNING:tensorflow:From <ipython-input-3-04fb87f54a18>:72: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Y_pred shape :  (?, 1)\n",
      "[step: 0] loss: 186099.078125\n",
      "[step: 1] loss: 184170.6875\n",
      "[step: 2] loss: 182166.3125\n",
      "[step: 3] loss: 180009.484375\n",
      "[step: 4] loss: 177611.46875\n",
      "[step: 5] loss: 174847.171875\n",
      "[step: 6] loss: 171527.25\n",
      "[step: 7] loss: 167567.71875\n",
      "[step: 8] loss: 163002.234375\n",
      "[step: 9] loss: 157932.640625\n",
      "[step: 10] loss: 152518.828125\n",
      "[step: 11] loss: 147120.453125\n",
      "[step: 12] loss: 142250.109375\n",
      "[step: 13] loss: 138306.171875\n",
      "[step: 14] loss: 135481.3125\n",
      "[step: 15] loss: 133398.78125\n",
      "[step: 16] loss: 131343.6875\n",
      "[step: 17] loss: 129105.015625\n",
      "[step: 18] loss: 126796.640625\n",
      "[step: 19] loss: 124531.203125\n",
      "[step: 20] loss: 122342.53125\n",
      "[step: 21] loss: 120202.0703125\n",
      "[step: 22] loss: 118063.921875\n",
      "[step: 23] loss: 115905.1015625\n",
      "[step: 24] loss: 113748.3984375\n",
      "[step: 25] loss: 111678.328125\n",
      "[step: 26] loss: 109822.6953125\n",
      "[step: 27] loss: 108273.9765625\n",
      "[step: 28] loss: 107056.640625\n",
      "[step: 29] loss: 106108.4765625\n",
      "[step: 30] loss: 105297.2265625\n",
      "[step: 31] loss: 104503.2578125\n",
      "[step: 32] loss: 103672.7421875\n",
      "[step: 33] loss: 102769.9921875\n",
      "[step: 34] loss: 101776.671875\n",
      "[step: 35] loss: 100671.9375\n",
      "[step: 36] loss: 99443.5546875\n",
      "[step: 37] loss: 98088.3828125\n",
      "[step: 38] loss: 96637.8828125\n",
      "[step: 39] loss: 95148.515625\n",
      "[step: 40] loss: 93752.8359375\n",
      "[step: 41] loss: 92671.125\n",
      "[step: 42] loss: 92103.4609375\n",
      "[step: 43] loss: 92008.40625\n",
      "[step: 44] loss: 92072.59375\n",
      "[step: 45] loss: 92065.3046875\n",
      "[step: 46] loss: 91983.0625\n",
      "[step: 47] loss: 91881.265625\n",
      "[step: 48] loss: 91781.671875\n",
      "[step: 49] loss: 91671.2421875\n",
      "[step: 50] loss: 91548.1875\n",
      "[step: 51] loss: 91418.8828125\n",
      "[step: 52] loss: 91283.7421875\n",
      "[step: 53] loss: 91139.8359375\n",
      "[step: 54] loss: 90977.71875\n",
      "[step: 55] loss: 90785.859375\n",
      "[step: 56] loss: 90541.203125\n",
      "[step: 57] loss: 90217.09375\n",
      "[step: 58] loss: 89788.59375\n",
      "[step: 59] loss: 89260.5625\n",
      "[step: 60] loss: 88691.3359375\n",
      "[step: 61] loss: 88184.7265625\n",
      "[step: 62] loss: 87837.0\n",
      "[step: 63] loss: 87675.2890625\n",
      "[step: 64] loss: 87644.6015625\n",
      "[step: 65] loss: 87666.65625\n",
      "[step: 66] loss: 87673.5\n",
      "[step: 67] loss: 87626.5546875\n",
      "[step: 68] loss: 87512.484375\n",
      "[step: 69] loss: 87335.75\n",
      "[step: 70] loss: 87113.7421875\n",
      "[step: 71] loss: 86870.1640625\n",
      "[step: 72] loss: 86631.671875\n",
      "[step: 73] loss: 86418.203125\n",
      "[step: 74] loss: 86248.5546875\n",
      "[step: 75] loss: 86124.546875\n",
      "[step: 76] loss: 86036.3515625\n",
      "[step: 77] loss: 85962.796875\n",
      "[step: 78] loss: 85881.203125\n",
      "[step: 79] loss: 85770.0546875\n",
      "[step: 80] loss: 85613.25\n",
      "[step: 81] loss: 85399.8359375\n",
      "[step: 82] loss: 85125.015625\n",
      "[step: 83] loss: 84796.40625\n",
      "[step: 84] loss: 84442.625\n",
      "[step: 85] loss: 84106.1171875\n",
      "[step: 86] loss: 83832.4609375\n",
      "[step: 87] loss: 83649.453125\n",
      "[step: 88] loss: 83545.703125\n",
      "[step: 89] loss: 83468.8203125\n",
      "[step: 90] loss: 83371.0390625\n",
      "[step: 91] loss: 83245.9609375\n",
      "[step: 92] loss: 83105.6796875\n",
      "[step: 93] loss: 82965.375\n",
      "[step: 94] loss: 82834.875\n",
      "[step: 95] loss: 82717.9140625\n",
      "[step: 96] loss: 82609.796875\n",
      "[step: 97] loss: 82503.0546875\n",
      "[step: 98] loss: 82391.7578125\n",
      "[step: 99] loss: 82275.203125\n",
      "[step: 100] loss: 82158.6015625\n",
      "[step: 101] loss: 82047.3046875\n",
      "[step: 102] loss: 81945.8125\n",
      "[step: 103] loss: 81855.0078125\n",
      "[step: 104] loss: 81773.875\n",
      "[step: 105] loss: 81698.59375\n",
      "[step: 106] loss: 81623.875\n",
      "[step: 107] loss: 81545.328125\n",
      "[step: 108] loss: 81462.125\n",
      "[step: 109] loss: 81376.71875\n",
      "[step: 110] loss: 81292.4609375\n",
      "[step: 111] loss: 81212.328125\n",
      "[step: 112] loss: 81136.5234375\n",
      "[step: 113] loss: 81064.515625\n",
      "[step: 114] loss: 80994.4375\n",
      "[step: 115] loss: 80924.828125\n",
      "[step: 116] loss: 80855.0\n",
      "[step: 117] loss: 80785.5703125\n",
      "[step: 118] loss: 80716.6953125\n",
      "[step: 119] loss: 80649.8515625\n",
      "[step: 120] loss: 80585.125\n",
      "[step: 121] loss: 80522.2265625\n",
      "[step: 122] loss: 80459.59375\n",
      "[step: 123] loss: 80396.578125\n",
      "[step: 124] loss: 80332.578125\n",
      "[step: 125] loss: 80267.953125\n",
      "[step: 126] loss: 80203.5234375\n",
      "[step: 127] loss: 80139.703125\n",
      "[step: 128] loss: 80076.25\n",
      "[step: 129] loss: 80012.59375\n",
      "[step: 130] loss: 79947.921875\n",
      "[step: 131] loss: 79881.46875\n",
      "[step: 132] loss: 79812.546875\n",
      "[step: 133] loss: 79739.96875\n",
      "[step: 134] loss: 79663.0\n",
      "[step: 135] loss: 79579.140625\n",
      "[step: 136] loss: 79485.5859375\n",
      "[step: 137] loss: 79377.4609375\n",
      "[step: 138] loss: 79248.9609375\n",
      "[step: 139] loss: 79090.71875\n",
      "[step: 140] loss: 78894.109375\n",
      "[step: 141] loss: 78645.671875\n",
      "[step: 142] loss: 78327.3984375\n",
      "[step: 143] loss: 77932.4921875\n",
      "[step: 144] loss: 77551.1796875\n",
      "[step: 145] loss: 77515.8046875\n",
      "[step: 146] loss: 77513.890625\n",
      "[step: 147] loss: 77212.5234375\n",
      "[step: 148] loss: 76902.5078125\n",
      "[step: 149] loss: 76773.7421875\n",
      "[step: 150] loss: 76755.9453125\n",
      "[step: 151] loss: 76721.4296875\n",
      "[step: 152] loss: 76617.75\n",
      "[step: 153] loss: 76437.7421875\n",
      "[step: 154] loss: 76210.6796875\n",
      "[step: 155] loss: 75980.828125\n",
      "[step: 156] loss: 75782.890625\n",
      "[step: 157] loss: 75594.9140625\n",
      "[step: 158] loss: 75387.1875\n",
      "[step: 159] loss: 75253.1328125\n",
      "[step: 160] loss: 75024.8828125\n",
      "[step: 161] loss: 74886.28125\n",
      "[step: 162] loss: 74842.546875\n",
      "[step: 163] loss: 74778.0859375\n",
      "[step: 164] loss: 74694.7890625\n",
      "[step: 165] loss: 74595.9375\n",
      "[step: 166] loss: 74493.078125\n",
      "[step: 167] loss: 74399.3984375\n",
      "[step: 168] loss: 74321.9765625\n",
      "[step: 169] loss: 74239.8125\n",
      "[step: 170] loss: 74132.0078125\n",
      "[step: 171] loss: 73990.1875\n",
      "[step: 172] loss: 73862.21875\n",
      "[step: 173] loss: 73805.234375\n",
      "[step: 174] loss: 73729.984375\n",
      "[step: 175] loss: 73598.4453125\n",
      "[step: 176] loss: 73478.109375\n",
      "[step: 177] loss: 73424.2421875\n",
      "[step: 178] loss: 73348.2578125\n",
      "[step: 179] loss: 73216.34375\n",
      "[step: 180] loss: 73123.4765625\n",
      "[step: 181] loss: 73066.03125\n",
      "[step: 182] loss: 72975.390625\n",
      "[step: 183] loss: 72869.1484375\n",
      "[step: 184] loss: 72819.4609375\n",
      "[step: 185] loss: 72745.1796875\n",
      "[step: 186] loss: 72642.546875\n",
      "[step: 187] loss: 72581.671875\n",
      "[step: 188] loss: 72502.1796875\n",
      "[step: 189] loss: 72404.84375\n",
      "[step: 190] loss: 72341.9609375\n",
      "[step: 191] loss: 72274.0390625\n",
      "[step: 192] loss: 72188.4375\n",
      "[step: 193] loss: 72124.609375\n",
      "[step: 194] loss: 72061.0\n",
      "[step: 195] loss: 71979.125\n",
      "[step: 196] loss: 71908.2421875\n",
      "[step: 197] loss: 71844.734375\n",
      "[step: 198] loss: 71767.109375\n",
      "[step: 199] loss: 71698.2890625\n",
      "[step: 200] loss: 71634.6796875\n",
      "[step: 201] loss: 71560.8515625\n",
      "[step: 202] loss: 71488.9453125\n",
      "[step: 203] loss: 71423.3828125\n",
      "[step: 204] loss: 71346.859375\n",
      "[step: 205] loss: 71279.7578125\n",
      "[step: 206] loss: 71211.5546875\n",
      "[step: 207] loss: 71139.3828125\n",
      "[step: 208] loss: 71074.3828125\n",
      "[step: 209] loss: 71000.8984375\n",
      "[step: 210] loss: 70934.5390625\n",
      "[step: 211] loss: 70863.34375\n",
      "[step: 212] loss: 70795.4921875\n",
      "[step: 213] loss: 70725.9921875\n",
      "[step: 214] loss: 70655.3828125\n",
      "[step: 215] loss: 70586.03125\n",
      "[step: 216] loss: 70512.171875\n",
      "[step: 217] loss: 70440.984375\n",
      "[step: 218] loss: 70363.015625\n",
      "[step: 219] loss: 70286.8359375\n",
      "[step: 220] loss: 70201.3984375\n",
      "[step: 221] loss: 70112.984375\n",
      "[step: 222] loss: 70011.2578125\n",
      "[step: 223] loss: 69898.453125\n",
      "[step: 224] loss: 69768.0625\n",
      "[step: 225] loss: 69622.046875\n",
      "[step: 226] loss: 69470.1328125\n",
      "[step: 227] loss: 69316.6953125\n",
      "[step: 228] loss: 69175.6796875\n",
      "[step: 229] loss: 69054.1484375\n",
      "[step: 230] loss: 68949.1796875\n",
      "[step: 231] loss: 68856.859375\n",
      "[step: 232] loss: 68774.8828125\n",
      "[step: 233] loss: 68699.5\n",
      "[step: 234] loss: 68629.609375\n",
      "[step: 235] loss: 68564.1953125\n",
      "[step: 236] loss: 68507.6796875\n",
      "[step: 237] loss: 68453.234375\n",
      "[step: 238] loss: 68396.46875\n",
      "[step: 239] loss: 68315.75\n",
      "[step: 240] loss: 68242.34375\n",
      "[step: 241] loss: 68187.9375\n",
      "[step: 242] loss: 68134.0546875\n",
      "[step: 243] loss: 68079.7421875\n",
      "[step: 244] loss: 68003.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 245] loss: 67927.2734375\n",
      "[step: 246] loss: 67855.25\n",
      "[step: 247] loss: 67787.8515625\n",
      "[step: 248] loss: 67722.328125\n",
      "[step: 249] loss: 67657.5625\n",
      "[step: 250] loss: 67596.8359375\n",
      "[step: 251] loss: 67561.671875\n",
      "[step: 252] loss: 67641.2109375\n",
      "[step: 253] loss: 67845.0\n",
      "[step: 254] loss: 67370.34375\n",
      "[step: 255] loss: 67614.53125\n",
      "[step: 256] loss: 67440.1640625\n",
      "[step: 257] loss: 67375.125\n",
      "[step: 258] loss: 67362.671875\n",
      "[step: 259] loss: 67182.078125\n",
      "[step: 260] loss: 67267.7578125\n",
      "[step: 261] loss: 67071.6015625\n",
      "[step: 262] loss: 67189.6484375\n",
      "[step: 263] loss: 66951.0546875\n",
      "[step: 264] loss: 67074.1796875\n",
      "[step: 265] loss: 66866.1796875\n",
      "[step: 266] loss: 66967.7265625\n",
      "[step: 267] loss: 66808.109375\n",
      "[step: 268] loss: 66821.71875\n",
      "[step: 269] loss: 66753.671875\n",
      "[step: 270] loss: 66691.9453125\n",
      "[step: 271] loss: 66698.6484375\n",
      "[step: 272] loss: 66583.265625\n",
      "[step: 273] loss: 66626.8828125\n",
      "[step: 274] loss: 66508.4921875\n",
      "[step: 275] loss: 66507.734375\n",
      "[step: 276] loss: 66449.7578125\n",
      "[step: 277] loss: 66383.9140625\n",
      "[step: 278] loss: 66383.0078125\n",
      "[step: 279] loss: 66300.2265625\n",
      "[step: 280] loss: 66281.0078125\n",
      "[step: 281] loss: 66258.3828125\n",
      "[step: 282] loss: 66176.921875\n",
      "[step: 283] loss: 66164.8359375\n",
      "[step: 284] loss: 66136.4765625\n",
      "[step: 285] loss: 66060.2109375\n",
      "[step: 286] loss: 66036.703125\n",
      "[step: 287] loss: 66016.2109375\n",
      "[step: 288] loss: 65952.3125\n",
      "[step: 289] loss: 65906.140625\n",
      "[step: 290] loss: 65887.8046875\n",
      "[step: 291] loss: 65856.671875\n",
      "[step: 292] loss: 65798.578125\n",
      "[step: 293] loss: 65750.8671875\n",
      "[step: 294] loss: 65723.5625\n",
      "[step: 295] loss: 65689.84375\n",
      "[step: 296] loss: 65645.53125\n",
      "[step: 297] loss: 65601.71875\n",
      "[step: 298] loss: 65567.390625\n",
      "[step: 299] loss: 65541.1640625\n",
      "[step: 300] loss: 65513.05859375\n",
      "[step: 301] loss: 65493.3828125\n",
      "[step: 302] loss: 65483.2265625\n",
      "[step: 303] loss: 65501.578125\n",
      "[step: 304] loss: 65575.921875\n",
      "[step: 305] loss: 65596.0234375\n",
      "[step: 306] loss: 65551.5703125\n",
      "[step: 307] loss: 65305.078125\n",
      "[step: 308] loss: 65257.70703125\n",
      "[step: 309] loss: 65383.74609375\n",
      "[step: 310] loss: 65348.6328125\n",
      "[step: 311] loss: 65191.0546875\n",
      "[step: 312] loss: 65123.046875\n",
      "[step: 313] loss: 65198.65234375\n",
      "[step: 314] loss: 65223.80859375\n",
      "[step: 315] loss: 65075.24609375\n",
      "[step: 316] loss: 65013.41796875\n",
      "[step: 317] loss: 65066.4609375\n",
      "[step: 318] loss: 65039.02734375\n",
      "[step: 319] loss: 64954.34375\n",
      "[step: 320] loss: 64903.59375\n",
      "[step: 321] loss: 64922.4140625\n",
      "[step: 322] loss: 64931.7421875\n",
      "[step: 323] loss: 64863.09765625\n",
      "[step: 324] loss: 64798.3984375\n",
      "[step: 325] loss: 64785.96484375\n",
      "[step: 326] loss: 64805.1796875\n",
      "[step: 327] loss: 64804.890625\n",
      "[step: 328] loss: 64745.98828125\n",
      "[step: 329] loss: 64685.43359375\n",
      "[step: 330] loss: 64642.04296875\n",
      "[step: 331] loss: 64629.171875\n",
      "[step: 332] loss: 64635.84765625\n",
      "[step: 333] loss: 64628.3359375\n",
      "[step: 334] loss: 64638.5703125\n",
      "[step: 335] loss: 64619.55078125\n",
      "[step: 336] loss: 64602.55859375\n",
      "[step: 337] loss: 64551.94921875\n",
      "[step: 338] loss: 64515.72265625\n",
      "[step: 339] loss: 64473.953125\n",
      "[step: 340] loss: 64465.8515625\n",
      "[step: 341] loss: 64454.671875\n",
      "[step: 342] loss: 64469.69140625\n",
      "[step: 343] loss: 64499.73828125\n",
      "[step: 344] loss: 64545.73828125\n",
      "[step: 345] loss: 64494.45703125\n",
      "[step: 346] loss: 64418.79296875\n",
      "[step: 347] loss: 64314.43359375\n",
      "[step: 348] loss: 64251.02734375\n",
      "[step: 349] loss: 64230.6875\n",
      "[step: 350] loss: 64238.03125\n",
      "[step: 351] loss: 64264.19140625\n",
      "[step: 352] loss: 64296.14453125\n",
      "[step: 353] loss: 64314.55859375\n",
      "[step: 354] loss: 64287.6796875\n",
      "[step: 355] loss: 64221.10546875\n",
      "[step: 356] loss: 64139.79296875\n",
      "[step: 357] loss: 64092.03515625\n",
      "[step: 358] loss: 64090.08984375\n",
      "[step: 359] loss: 64128.5390625\n",
      "[step: 360] loss: 64177.8984375\n",
      "[step: 361] loss: 64182.78515625\n",
      "[step: 362] loss: 64113.30859375\n",
      "[step: 363] loss: 64020.03515625\n",
      "[step: 364] loss: 63981.66796875\n",
      "[step: 365] loss: 64016.41015625\n",
      "[step: 366] loss: 64082.921875\n",
      "[step: 367] loss: 64079.84765625\n",
      "[step: 368] loss: 63990.453125\n",
      "[step: 369] loss: 63919.84375\n",
      "[step: 370] loss: 63893.48046875\n",
      "[step: 371] loss: 63893.09375\n",
      "[step: 372] loss: 63897.3359375\n",
      "[step: 373] loss: 63889.71484375\n",
      "[step: 374] loss: 63877.24609375\n",
      "[step: 375] loss: 63849.6015625\n",
      "[step: 376] loss: 63821.078125\n",
      "[step: 377] loss: 63805.78515625\n",
      "[step: 378] loss: 63787.765625\n",
      "[step: 379] loss: 63771.26953125\n",
      "[step: 380] loss: 63758.890625\n",
      "[step: 381] loss: 63750.2890625\n",
      "[step: 382] loss: 63746.6796875\n",
      "[step: 383] loss: 63743.5078125\n",
      "[step: 384] loss: 63749.30078125\n",
      "[step: 385] loss: 63759.82421875\n",
      "[step: 386] loss: 63763.58984375\n",
      "[step: 387] loss: 63762.8203125\n",
      "[step: 388] loss: 63755.58203125\n",
      "[step: 389] loss: 63733.37109375\n",
      "[step: 390] loss: 63699.01953125\n",
      "[step: 391] loss: 63663.3125\n",
      "[step: 392] loss: 63625.0546875\n",
      "[step: 393] loss: 63604.609375\n",
      "[step: 394] loss: 63598.91796875\n",
      "[step: 395] loss: 63603.76953125\n",
      "[step: 396] loss: 63608.04296875\n",
      "[step: 397] loss: 63608.94140625\n",
      "[step: 398] loss: 63601.46875\n",
      "[step: 399] loss: 63585.6875\n",
      "[step: 400] loss: 63563.81640625\n",
      "[step: 401] loss: 63534.671875\n",
      "[step: 402] loss: 63516.48828125\n",
      "[step: 403] loss: 63502.34375\n",
      "[step: 404] loss: 63493.23828125\n",
      "[step: 405] loss: 63490.48828125\n",
      "[step: 406] loss: 63489.734375\n",
      "[step: 407] loss: 63496.02734375\n",
      "[step: 408] loss: 63497.53515625\n",
      "[step: 409] loss: 63520.98046875\n",
      "[step: 410] loss: 63551.93359375\n",
      "[step: 411] loss: 63597.578125\n",
      "[step: 412] loss: 63605.21875\n",
      "[step: 413] loss: 63653.2109375\n",
      "[step: 414] loss: 63609.921875\n",
      "[step: 415] loss: 63554.125\n",
      "[step: 416] loss: 63420.8984375\n",
      "[step: 417] loss: 63381.7578125\n",
      "[step: 418] loss: 63429.77734375\n",
      "[step: 419] loss: 63459.41796875\n",
      "[step: 420] loss: 63446.2890625\n",
      "[step: 421] loss: 63366.01171875\n",
      "[step: 422] loss: 63336.1640625\n",
      "[step: 423] loss: 63338.74609375\n",
      "[step: 424] loss: 63343.5390625\n",
      "[step: 425] loss: 63353.1015625\n",
      "[step: 426] loss: 63327.0859375\n",
      "[step: 427] loss: 63307.12890625\n",
      "[step: 428] loss: 63290.6953125\n",
      "[step: 429] loss: 63281.546875\n",
      "[step: 430] loss: 63275.46484375\n",
      "[step: 431] loss: 63272.85546875\n",
      "[step: 432] loss: 63274.859375\n",
      "[step: 433] loss: 63263.7734375\n",
      "[step: 434] loss: 63255.90625\n",
      "[step: 435] loss: 63241.390625\n",
      "[step: 436] loss: 63231.3671875\n",
      "[step: 437] loss: 63221.68359375\n",
      "[step: 438] loss: 63213.828125\n",
      "[step: 439] loss: 63206.38671875\n",
      "[step: 440] loss: 63198.9921875\n",
      "[step: 441] loss: 63191.88671875\n",
      "[step: 442] loss: 63185.31640625\n",
      "[step: 443] loss: 63180.44140625\n",
      "[step: 444] loss: 63179.08203125\n",
      "[step: 445] loss: 63180.73828125\n",
      "[step: 446] loss: 63184.09375\n",
      "[step: 447] loss: 63185.4921875\n",
      "[step: 448] loss: 63177.94921875\n",
      "[step: 449] loss: 63167.4921875\n",
      "[step: 450] loss: 63154.2109375\n",
      "[step: 451] loss: 63137.6953125\n",
      "[step: 452] loss: 63129.33984375\n",
      "[step: 453] loss: 63121.8984375\n",
      "[step: 454] loss: 63122.22265625\n",
      "[step: 455] loss: 63117.41796875\n",
      "[step: 456] loss: 63112.3515625\n",
      "[step: 457] loss: 63102.28125\n",
      "[step: 458] loss: 63093.078125\n",
      "[step: 459] loss: 63084.44140625\n",
      "[step: 460] loss: 63078.05078125\n",
      "[step: 461] loss: 63073.5\n",
      "[step: 462] loss: 63070.21875\n",
      "[step: 463] loss: 63065.26171875\n",
      "[step: 464] loss: 63061.02734375\n",
      "[step: 465] loss: 63057.765625\n",
      "[step: 466] loss: 63059.953125\n",
      "[step: 467] loss: 63062.546875\n",
      "[step: 468] loss: 63094.234375\n",
      "[step: 469] loss: 63148.36328125\n",
      "[step: 470] loss: 63227.39453125\n",
      "[step: 471] loss: 63400.48046875\n",
      "[step: 472] loss: 63373.33203125\n",
      "[step: 473] loss: 63301.8125\n",
      "[step: 474] loss: 63040.3515625\n",
      "[step: 475] loss: 62995.45703125\n",
      "[step: 476] loss: 63165.3828125\n",
      "[step: 477] loss: 63178.9375\n",
      "[step: 478] loss: 63058.55859375\n",
      "[step: 479] loss: 62951.828125\n",
      "[step: 480] loss: 63024.20703125\n",
      "[step: 481] loss: 63107.89453125\n",
      "[step: 482] loss: 63012.4140625\n",
      "[step: 483] loss: 62926.3515625\n",
      "[step: 484] loss: 62956.078125\n",
      "[step: 485] loss: 63018.796875\n",
      "[step: 486] loss: 63011.6640625\n",
      "[step: 487] loss: 62929.6015625\n",
      "[step: 488] loss: 62899.3671875\n",
      "[step: 489] loss: 62945.58984375\n",
      "[step: 490] loss: 62976.82421875\n",
      "[step: 491] loss: 62943.734375\n",
      "[step: 492] loss: 62885.76171875\n",
      "[step: 493] loss: 62869.72265625\n",
      "[step: 494] loss: 62906.55078125\n",
      "[step: 495] loss: 62915.1015625\n",
      "[step: 496] loss: 62881.90234375\n",
      "[step: 497] loss: 62844.3359375\n",
      "[step: 498] loss: 62846.7265625\n",
      "[step: 499] loss: 62864.01953125\n",
      "test_predict shape :  (12583, 1)\n",
      "RMAE: 2.1260509490966797\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 10\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.005\n",
    "iterations = 500\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "#xy = np.loadtxt('preprocessed/X_train_15K.csv', delimiter=',', usecols=(1,2,3,4,5,6,7,8,9,10,11,12,13))\n",
    "xy = np.loadtxt('preprocessed/X_train_15K.csv', delimiter=',', usecols=(0,1,2,3))\n",
    "y = np.loadtxt('preprocessed/y_train_15K.csv', delimiter = ',')\n",
    "y = np.reshape(y, (len(y),1))\n",
    "#print(xy)\n",
    "#print(y)\n",
    "print(\"xy shape : \", xy.shape, \"y shape : \",y.shape)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_setx = xy[0:train_size]\n",
    "train_sety = y[0:train_size]\n",
    "test_setx = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "test_sety = y[train_size - seq_length:]\n",
    "\n",
    "# Scale each\n",
    "# train_setx = MinMaxScaler(train_setx)\n",
    "# train_sety = MinMaxScaler(train_sety)\n",
    "# test_setx = MinMaxScaler(test_setx)\n",
    "# test_sety = MinMaxScaler(test_sety)\n",
    "print(\"train_setx shape : \", train_setx.shape, \"train_sety shape : \", train_sety.shape)\n",
    "print(\"test_setx shape : \", test_setx.shape, \"test_sety shape : \", test_sety.shape)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(XX, YY, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(XX) - seq_length):\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        _y = YY[i+seq_length]\n",
    "\n",
    "        #print(\"_x shape : \", _x.shape)\n",
    "        #print(\"_y shape : \", _y.shape)\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_setx, train_sety, seq_length)\n",
    "testX, testY = build_dataset(test_setx, test_sety, seq_length)\n",
    "finalX, finalY = build_dataset(xy, y, seq_length)\n",
    "# print(\"trainX\")\n",
    "# print(trainX)\n",
    "# print(\"trainY\")\n",
    "# print(trainY)\n",
    "# print(\"testX\")\n",
    "# print(testX)\n",
    "# print(\"testY\")\n",
    "# print(testY)\n",
    "print(\"trainX shape : \", trainX.shape, \"trainY shape : \", trainY.shape)\n",
    "print(\"testX shape : \", testX.shape, \"testY shape : \", testY.shape)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "print(\"Y_pred shape : \", Y_pred.shape)\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.abs(Y_pred - Y))  # sum of the absolute values 이거 reduced_mean으로 바꿔봤는데 오차 증가함\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "rmae = tf.reduce_mean(tf.abs(targets - predictions))\n",
    "\n",
    "sess = tf.Session() \n",
    "final_sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "final_sess.run(init)\n",
    "\n",
    "# Training step\n",
    "for i in range(iterations):\n",
    "    _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                            X: trainX, Y: trainY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        \n",
    "#     scores = cross_validate(reg_lin, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "#     print(scores['test_score'])\n",
    "#     print(\"{:.4f}\".format(sum(scores['test_score']) / 5))\n",
    "\n",
    "# Test step\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "print(\"test_predict shape : \", test_predict.shape)\n",
    "# rmse_val = sess.run(rmse, feed_dict={\n",
    "#                 targets: testY, predictions: test_predict})\n",
    "# print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "rmae_val = sess.run(rmae, feed_dict={\n",
    "                targets: testY, predictions: test_predict})\n",
    "print(\"RMAE: {}\".format(rmae_val))\n",
    "\n",
    "\n",
    "    \n",
    "# Plot predictions\n",
    "plt.plot(testY)\n",
    "plt.plot(test_predict)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Time To Failure\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 230533.4375\n",
      "[step: 1] loss: 229733.328125\n",
      "[step: 2] loss: 228961.734375\n",
      "[step: 3] loss: 228194.5\n",
      "[step: 4] loss: 227404.203125\n",
      "[step: 5] loss: 226563.234375\n",
      "[step: 6] loss: 225642.140625\n",
      "[step: 7] loss: 224615.5625\n",
      "[step: 8] loss: 223451.75\n",
      "[step: 9] loss: 222114.171875\n",
      "[step: 10] loss: 220559.296875\n",
      "[step: 11] loss: 218739.328125\n",
      "[step: 12] loss: 216609.71875\n",
      "[step: 13] loss: 214151.484375\n",
      "[step: 14] loss: 211419.15625\n",
      "[step: 15] loss: 208588.671875\n",
      "[step: 16] loss: 205887.46875\n",
      "[step: 17] loss: 203399.21875\n",
      "[step: 18] loss: 200967.546875\n",
      "[step: 19] loss: 198379.515625\n",
      "[step: 20] loss: 195466.0625\n",
      "[step: 21] loss: 192126.125\n",
      "[step: 22] loss: 188321.921875\n",
      "[step: 23] loss: 184122.46875\n",
      "[step: 24] loss: 179719.078125\n",
      "[step: 25] loss: 175452.1875\n",
      "[step: 26] loss: 171692.90625\n",
      "[step: 27] loss: 168713.3125\n",
      "[step: 28] loss: 166467.671875\n",
      "[step: 29] loss: 164751.984375\n",
      "[step: 30] loss: 163322.671875\n",
      "[step: 31] loss: 162035.4375\n",
      "[step: 32] loss: 160772.640625\n",
      "[step: 33] loss: 159460.984375\n",
      "[step: 34] loss: 158029.90625\n",
      "[step: 35] loss: 156422.9375\n",
      "[step: 36] loss: 154573.875\n",
      "[step: 37] loss: 152412.546875\n",
      "[step: 38] loss: 149855.5\n",
      "[step: 39] loss: 146845.8125\n",
      "[step: 40] loss: 143465.53125\n",
      "[step: 41] loss: 140130.453125\n",
      "[step: 42] loss: 137312.75\n",
      "[step: 43] loss: 134930.171875\n",
      "[step: 44] loss: 132781.640625\n",
      "[step: 45] loss: 130753.5703125\n",
      "[step: 46] loss: 128875.15625\n",
      "[step: 47] loss: 127252.2578125\n",
      "[step: 48] loss: 126080.375\n",
      "[step: 49] loss: 125464.3125\n",
      "[step: 50] loss: 125234.8203125\n",
      "[step: 51] loss: 125191.1875\n",
      "[step: 52] loss: 125238.0859375\n",
      "[step: 53] loss: 125312.2265625\n",
      "[step: 54] loss: 125350.1484375\n",
      "[step: 55] loss: 125297.5859375\n",
      "[step: 56] loss: 125147.828125\n",
      "[step: 57] loss: 124918.1875\n",
      "[step: 58] loss: 124636.8671875\n",
      "[step: 59] loss: 124336.171875\n",
      "[step: 60] loss: 124049.078125\n",
      "[step: 61] loss: 123802.890625\n",
      "[step: 62] loss: 123612.5859375\n",
      "[step: 63] loss: 123480.375\n",
      "[step: 64] loss: 123395.7421875\n",
      "[step: 65] loss: 123341.453125\n",
      "[step: 66] loss: 123298.7109375\n",
      "[step: 67] loss: 123251.6171875\n",
      "[step: 68] loss: 123189.828125\n",
      "[step: 69] loss: 123107.7734375\n",
      "[step: 70] loss: 123004.25\n",
      "[step: 71] loss: 122881.1484375\n",
      "[step: 72] loss: 122741.8984375\n",
      "[step: 73] loss: 122588.390625\n",
      "[step: 74] loss: 122424.4375\n",
      "[step: 75] loss: 122252.203125\n",
      "[step: 76] loss: 122080.3515625\n",
      "[step: 77] loss: 121929.125\n",
      "[step: 78] loss: 121832.40625\n",
      "[step: 79] loss: 121754.2265625\n",
      "[step: 80] loss: 121608.9921875\n",
      "[step: 81] loss: 121385.5625\n",
      "[step: 82] loss: 121113.984375\n",
      "[step: 83] loss: 120820.765625\n",
      "[step: 84] loss: 120507.25\n",
      "[step: 85] loss: 120160.328125\n",
      "[step: 86] loss: 119745.4453125\n",
      "[step: 87] loss: 119267.7109375\n",
      "[step: 88] loss: 118766.3828125\n",
      "[step: 89] loss: 118281.6015625\n",
      "[step: 90] loss: 117949.9296875\n",
      "[step: 91] loss: 117864.9921875\n",
      "[step: 92] loss: 117700.203125\n",
      "[step: 93] loss: 117428.65625\n",
      "[step: 94] loss: 117071.5078125\n",
      "[step: 95] loss: 116677.84375\n",
      "[step: 96] loss: 116281.8984375\n",
      "[step: 97] loss: 115885.875\n",
      "[step: 98] loss: 115483.1875\n",
      "[step: 99] loss: 115119.125\n",
      "[step: 100] loss: 114835.6796875\n",
      "[step: 101] loss: 114739.7265625\n",
      "[step: 102] loss: 114650.2890625\n",
      "[step: 103] loss: 114388.4296875\n",
      "[step: 104] loss: 114061.46875\n",
      "[step: 105] loss: 113756.421875\n",
      "[step: 106] loss: 113530.375\n",
      "[step: 107] loss: 113352.46875\n",
      "[step: 108] loss: 113163.6015625\n",
      "[step: 109] loss: 112952.109375\n",
      "[step: 110] loss: 112717.0625\n",
      "[step: 111] loss: 112466.796875\n",
      "[step: 112] loss: 112236.5078125\n",
      "[step: 113] loss: 112046.8359375\n",
      "[step: 114] loss: 111875.1328125\n",
      "[step: 115] loss: 111699.609375\n",
      "[step: 116] loss: 111509.7578125\n",
      "[step: 117] loss: 111297.421875\n",
      "[step: 118] loss: 111077.0078125\n",
      "[step: 119] loss: 110874.1015625\n",
      "[step: 120] loss: 110691.4375\n",
      "[step: 121] loss: 110515.59375\n",
      "[step: 122] loss: 110341.21875\n",
      "[step: 123] loss: 110158.671875\n",
      "[step: 124] loss: 109965.3359375\n",
      "[step: 125] loss: 109778.15625\n",
      "[step: 126] loss: 109603.5\n",
      "[step: 127] loss: 109434.7578125\n",
      "[step: 128] loss: 109270.3515625\n",
      "[step: 129] loss: 109098.6640625\n",
      "[step: 130] loss: 108919.078125\n",
      "[step: 131] loss: 108743.765625\n",
      "[step: 132] loss: 108574.828125\n",
      "[step: 133] loss: 108411.046875\n",
      "[step: 134] loss: 108248.6015625\n",
      "[step: 135] loss: 108080.6328125\n",
      "[step: 136] loss: 107911.0625\n",
      "[step: 137] loss: 107746.328125\n",
      "[step: 138] loss: 107583.875\n",
      "[step: 139] loss: 107423.7890625\n",
      "[step: 140] loss: 107259.0546875\n",
      "[step: 141] loss: 107090.84375\n",
      "[step: 142] loss: 106925.5078125\n",
      "[step: 143] loss: 106761.34375\n",
      "[step: 144] loss: 106596.8046875\n",
      "[step: 145] loss: 106427.9140625\n",
      "[step: 146] loss: 106254.8828125\n",
      "[step: 147] loss: 106083.6171875\n",
      "[step: 148] loss: 105912.875\n",
      "[step: 149] loss: 105738.875\n",
      "[step: 150] loss: 105560.1015625\n",
      "[step: 151] loss: 105380.1796875\n",
      "[step: 152] loss: 105201.6796875\n",
      "[step: 153] loss: 105019.1015625\n",
      "[step: 154] loss: 104832.359375\n",
      "[step: 155] loss: 104644.40625\n",
      "[step: 156] loss: 104455.5625\n",
      "[step: 157] loss: 104263.6328125\n",
      "[step: 158] loss: 104070.265625\n",
      "[step: 159] loss: 103877.8125\n",
      "[step: 160] loss: 103681.1484375\n",
      "[step: 161] loss: 103482.3515625\n",
      "[step: 162] loss: 103283.234375\n",
      "[step: 163] loss: 103083.0\n",
      "[step: 164] loss: 102881.4609375\n",
      "[step: 165] loss: 102681.9765625\n",
      "[step: 166] loss: 102479.4453125\n",
      "[step: 167] loss: 102275.71875\n",
      "[step: 168] loss: 102072.1171875\n",
      "[step: 169] loss: 101866.7890625\n",
      "[step: 170] loss: 101662.9609375\n",
      "[step: 171] loss: 101458.6015625\n",
      "[step: 172] loss: 101252.828125\n",
      "[step: 173] loss: 101047.5234375\n",
      "[step: 174] loss: 100843.3125\n",
      "[step: 175] loss: 100640.9765625\n",
      "[step: 176] loss: 100458.96875\n",
      "[step: 177] loss: 100446.9453125\n",
      "[step: 178] loss: 100443.453125\n",
      "[step: 179] loss: 100080.0390625\n",
      "[step: 180] loss: 99879.328125\n",
      "[step: 181] loss: 99826.9921875\n",
      "[step: 182] loss: 99428.2109375\n",
      "[step: 183] loss: 99488.1796875\n",
      "[step: 184] loss: 99114.9375\n",
      "[step: 185] loss: 99201.3828125\n",
      "[step: 186] loss: 98901.453125\n",
      "[step: 187] loss: 98796.4765625\n",
      "[step: 188] loss: 98646.8671875\n",
      "[step: 189] loss: 98447.28125\n",
      "[step: 190] loss: 98406.1875\n",
      "[step: 191] loss: 98157.9765625\n",
      "[step: 192] loss: 98150.984375\n",
      "[step: 193] loss: 97908.078125\n",
      "[step: 194] loss: 97834.953125\n",
      "[step: 195] loss: 97714.1484375\n",
      "[step: 196] loss: 97516.0546875\n",
      "[step: 197] loss: 97468.4609375\n",
      "[step: 198] loss: 97293.9921875\n",
      "[step: 199] loss: 97177.578125\n",
      "[step: 200] loss: 97102.2109375\n",
      "[step: 201] loss: 96930.609375\n",
      "[step: 202] loss: 96844.9609375\n",
      "[step: 203] loss: 96757.234375\n",
      "[step: 204] loss: 96603.625\n",
      "[step: 205] loss: 96500.328125\n",
      "[step: 206] loss: 96425.6796875\n",
      "[step: 207] loss: 96302.2265625\n",
      "[step: 208] loss: 96171.078125\n",
      "[step: 209] loss: 96091.8984375\n",
      "[step: 210] loss: 96012.2734375\n",
      "[step: 211] loss: 95900.6015625\n",
      "[step: 212] loss: 95775.1484375\n",
      "[step: 213] loss: 95678.9765625\n",
      "[step: 214] loss: 95604.2890625\n",
      "[step: 215] loss: 95524.328125\n",
      "[step: 216] loss: 95433.25\n",
      "[step: 217] loss: 95325.375\n",
      "[step: 218] loss: 95218.8046875\n",
      "[step: 219] loss: 95120.375\n",
      "[step: 220] loss: 95036.75\n",
      "[step: 221] loss: 94967.3671875\n",
      "[step: 222] loss: 94933.421875\n",
      "[step: 223] loss: 95076.984375\n",
      "[step: 224] loss: 95187.671875\n",
      "[step: 225] loss: 95450.578125\n",
      "[step: 226] loss: 94558.7421875\n",
      "[step: 227] loss: 95225.28125\n",
      "[step: 228] loss: 95421.078125\n",
      "[step: 229] loss: 94691.078125\n",
      "[step: 230] loss: 95456.78125\n",
      "[step: 231] loss: 94326.7734375\n",
      "[step: 232] loss: 94928.6953125\n",
      "[step: 233] loss: 94188.3828125\n",
      "[step: 234] loss: 94916.8203125\n",
      "[step: 235] loss: 94102.1015625\n",
      "[step: 236] loss: 94521.4140625\n",
      "[step: 237] loss: 93943.359375\n",
      "[step: 238] loss: 94502.125\n",
      "[step: 239] loss: 93882.96875\n",
      "[step: 240] loss: 94192.140625\n",
      "[step: 241] loss: 93722.109375\n",
      "[step: 242] loss: 94071.796875\n",
      "[step: 243] loss: 93710.984375\n",
      "[step: 244] loss: 93824.625\n",
      "[step: 245] loss: 93591.0859375\n",
      "[step: 246] loss: 93628.609375\n",
      "[step: 247] loss: 93622.6796875\n",
      "[step: 248] loss: 93461.6796875\n",
      "[step: 249] loss: 93623.71875\n",
      "[step: 250] loss: 93286.234375\n",
      "[step: 251] loss: 93477.6796875\n",
      "[step: 252] loss: 93215.6015625\n",
      "[step: 253] loss: 93315.609375\n",
      "[step: 254] loss: 93242.4375\n",
      "[step: 255] loss: 93115.015625\n",
      "[step: 256] loss: 93247.8828125\n",
      "[step: 257] loss: 93019.734375\n",
      "[step: 258] loss: 93053.3203125\n",
      "[step: 259] loss: 93023.65625\n",
      "[step: 260] loss: 92889.0390625\n",
      "[step: 261] loss: 92958.5234375\n",
      "[step: 262] loss: 92830.5703125\n",
      "[step: 263] loss: 92812.2421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 264] loss: 92806.5234375\n",
      "[step: 265] loss: 92702.1328125\n",
      "[step: 266] loss: 92713.078125\n",
      "[step: 267] loss: 92662.4140625\n",
      "[step: 268] loss: 92599.8359375\n",
      "[step: 269] loss: 92604.9921875\n",
      "[step: 270] loss: 92537.7109375\n",
      "[step: 271] loss: 92510.0625\n",
      "[step: 272] loss: 92499.4609375\n",
      "[step: 273] loss: 92431.328125\n",
      "[step: 274] loss: 92414.0546875\n",
      "[step: 275] loss: 92392.96875\n",
      "[step: 276] loss: 92332.25\n",
      "[step: 277] loss: 92316.734375\n",
      "[step: 278] loss: 92295.0\n",
      "[step: 279] loss: 92239.703125\n",
      "[step: 280] loss: 92220.984375\n",
      "[step: 281] loss: 92203.8515625\n",
      "[step: 282] loss: 92153.4453125\n",
      "[step: 283] loss: 92123.7890625\n",
      "[step: 284] loss: 92106.4453125\n",
      "[step: 285] loss: 92071.625\n",
      "[step: 286] loss: 92031.484375\n",
      "[step: 287] loss: 92011.703125\n",
      "[step: 288] loss: 91988.828125\n",
      "[step: 289] loss: 91951.7734375\n",
      "[step: 290] loss: 91919.6953125\n",
      "[step: 291] loss: 91898.859375\n",
      "[step: 292] loss: 91874.0390625\n",
      "[step: 293] loss: 91841.5078125\n",
      "[step: 294] loss: 91811.703125\n",
      "[step: 295] loss: 91790.140625\n",
      "[step: 296] loss: 91772.3671875\n",
      "[step: 297] loss: 91748.8125\n",
      "[step: 298] loss: 91724.8828125\n",
      "[step: 299] loss: 91704.5703125\n",
      "[step: 300] loss: 91693.78125\n",
      "[step: 301] loss: 91706.6171875\n",
      "[step: 302] loss: 91748.8828125\n",
      "[step: 303] loss: 91799.34375\n",
      "[step: 304] loss: 91971.1484375\n",
      "[step: 305] loss: 91958.1015625\n",
      "[step: 306] loss: 92068.5\n",
      "[step: 307] loss: 91666.859375\n",
      "[step: 308] loss: 91495.2578125\n",
      "[step: 309] loss: 91671.8984375\n",
      "[step: 310] loss: 91720.1328125\n",
      "[step: 311] loss: 91562.5703125\n",
      "[step: 312] loss: 91415.390625\n",
      "[step: 313] loss: 91496.5859375\n",
      "[step: 314] loss: 91602.0078125\n",
      "[step: 315] loss: 91457.828125\n",
      "[step: 316] loss: 91338.03125\n",
      "[step: 317] loss: 91334.6796875\n",
      "[step: 318] loss: 91404.515625\n",
      "[step: 319] loss: 91518.3359375\n",
      "[step: 320] loss: 91475.6953125\n",
      "[step: 321] loss: 91444.6640625\n",
      "[step: 322] loss: 91273.2265625\n",
      "[step: 323] loss: 91213.140625\n",
      "[step: 324] loss: 91260.8046875\n",
      "[step: 325] loss: 91297.0078125\n",
      "[step: 326] loss: 91306.6953125\n",
      "[step: 327] loss: 91182.5703125\n",
      "[step: 328] loss: 91134.90625\n",
      "[step: 329] loss: 91160.203125\n",
      "[step: 330] loss: 91161.8984375\n",
      "[step: 331] loss: 91155.9609375\n",
      "[step: 332] loss: 91099.5703125\n",
      "[step: 333] loss: 91061.0625\n",
      "[step: 334] loss: 91042.9453125\n",
      "[step: 335] loss: 91050.4453125\n",
      "[step: 336] loss: 91087.3359375\n",
      "[step: 337] loss: 91075.5546875\n",
      "[step: 338] loss: 91069.890625\n",
      "[step: 339] loss: 91017.125\n",
      "[step: 340] loss: 90995.234375\n",
      "[step: 341] loss: 90968.59375\n",
      "[step: 342] loss: 90955.8125\n",
      "[step: 343] loss: 90938.5703125\n",
      "[step: 344] loss: 90934.078125\n",
      "[step: 345] loss: 90936.3203125\n",
      "[step: 346] loss: 90971.890625\n",
      "[step: 347] loss: 90967.5\n",
      "[step: 348] loss: 91018.2890625\n",
      "[step: 349] loss: 90991.375\n",
      "[step: 350] loss: 91031.0234375\n",
      "[step: 351] loss: 90944.2890625\n",
      "[step: 352] loss: 90909.6796875\n",
      "[step: 353] loss: 90826.421875\n",
      "[step: 354] loss: 90782.078125\n",
      "[step: 355] loss: 90761.9921875\n",
      "[step: 356] loss: 90747.421875\n",
      "[step: 357] loss: 90737.203125\n",
      "[step: 358] loss: 90734.6953125\n",
      "[step: 359] loss: 90747.1171875\n",
      "[step: 360] loss: 90781.2734375\n",
      "[step: 361] loss: 90899.1796875\n",
      "[step: 362] loss: 90962.015625\n",
      "[step: 363] loss: 91180.28125\n",
      "[step: 364] loss: 90939.8671875\n",
      "[step: 365] loss: 90797.5078125\n",
      "[step: 366] loss: 90642.328125\n",
      "[step: 367] loss: 90673.2421875\n",
      "[step: 368] loss: 90837.984375\n",
      "[step: 369] loss: 90832.4375\n",
      "[step: 370] loss: 90780.5\n",
      "[step: 371] loss: 90617.0625\n",
      "[step: 372] loss: 90578.796875\n",
      "[step: 373] loss: 90634.1171875\n",
      "[step: 374] loss: 90698.2109375\n",
      "[step: 375] loss: 90795.921875\n",
      "[step: 376] loss: 90658.796875\n",
      "[step: 377] loss: 90574.203125\n",
      "[step: 378] loss: 90516.5703125\n",
      "[step: 379] loss: 90513.4609375\n",
      "[step: 380] loss: 90562.859375\n",
      "[step: 381] loss: 90610.8359375\n",
      "[step: 382] loss: 90690.6484375\n",
      "[step: 383] loss: 90620.796875\n",
      "[step: 384] loss: 90557.90625\n",
      "[step: 385] loss: 90463.328125\n",
      "[step: 386] loss: 90440.234375\n",
      "[step: 387] loss: 90485.2265625\n",
      "[step: 388] loss: 90533.9375\n",
      "[step: 389] loss: 90587.0625\n",
      "[step: 390] loss: 90511.1328125\n",
      "[step: 391] loss: 90445.359375\n",
      "[step: 392] loss: 90387.0\n",
      "[step: 393] loss: 90381.1796875\n",
      "[step: 394] loss: 90413.0859375\n",
      "[step: 395] loss: 90424.46875\n",
      "[step: 396] loss: 90431.421875\n",
      "[step: 397] loss: 90389.5390625\n",
      "[step: 398] loss: 90360.59375\n",
      "[step: 399] loss: 90322.7265625\n",
      "[step: 400] loss: 90306.7109375\n",
      "[step: 401] loss: 90323.734375\n",
      "[step: 402] loss: 90339.8359375\n",
      "[step: 403] loss: 90366.75\n",
      "[step: 404] loss: 90372.078125\n",
      "[step: 405] loss: 90389.7890625\n",
      "[step: 406] loss: 90341.6484375\n",
      "[step: 407] loss: 90303.7578125\n",
      "[step: 408] loss: 90254.140625\n",
      "[step: 409] loss: 90225.953125\n",
      "[step: 410] loss: 90217.2734375\n",
      "[step: 411] loss: 90228.4609375\n",
      "[step: 412] loss: 90261.25\n",
      "[step: 413] loss: 90275.6796875\n",
      "[step: 414] loss: 90295.625\n",
      "[step: 415] loss: 90277.2265625\n",
      "[step: 416] loss: 90249.4375\n",
      "[step: 417] loss: 90193.3828125\n",
      "[step: 418] loss: 90163.7109375\n",
      "[step: 419] loss: 90135.0703125\n",
      "[step: 420] loss: 90122.0390625\n",
      "[step: 421] loss: 90118.828125\n",
      "[step: 422] loss: 90122.078125\n",
      "[step: 423] loss: 90144.9296875\n",
      "[step: 424] loss: 90175.484375\n",
      "[step: 425] loss: 90222.3203125\n",
      "[step: 426] loss: 90239.0390625\n",
      "[step: 427] loss: 90307.296875\n",
      "[step: 428] loss: 90248.25\n",
      "[step: 429] loss: 90222.4453125\n",
      "[step: 430] loss: 90125.9609375\n",
      "[step: 431] loss: 90045.5078125\n",
      "[step: 432] loss: 90025.296875\n",
      "[step: 433] loss: 90061.6015625\n",
      "[step: 434] loss: 90120.7109375\n",
      "[step: 435] loss: 90123.59375\n",
      "[step: 436] loss: 90137.0625\n",
      "[step: 437] loss: 90056.171875\n",
      "[step: 438] loss: 89989.5\n",
      "[step: 439] loss: 89965.1796875\n",
      "[step: 440] loss: 89981.859375\n",
      "[step: 441] loss: 90012.5078125\n",
      "[step: 442] loss: 90026.6640625\n",
      "[step: 443] loss: 90039.125\n",
      "[step: 444] loss: 90004.03125\n",
      "[step: 445] loss: 89966.7109375\n",
      "[step: 446] loss: 89932.03125\n",
      "[step: 447] loss: 89901.2421875\n",
      "[step: 448] loss: 89896.6953125\n",
      "[step: 449] loss: 89915.421875\n",
      "[step: 450] loss: 89926.265625\n",
      "[step: 451] loss: 89930.625\n",
      "[step: 452] loss: 89929.8125\n",
      "[step: 453] loss: 89909.21875\n",
      "[step: 454] loss: 89887.0703125\n",
      "[step: 455] loss: 89863.5703125\n",
      "[step: 456] loss: 89833.9921875\n",
      "[step: 457] loss: 89821.1875\n",
      "[step: 458] loss: 89823.2890625\n",
      "[step: 459] loss: 89825.640625\n",
      "[step: 460] loss: 89829.15625\n",
      "[step: 461] loss: 89815.9765625\n",
      "[step: 462] loss: 89802.7421875\n",
      "[step: 463] loss: 89789.1640625\n",
      "[step: 464] loss: 89780.28125\n",
      "[step: 465] loss: 89769.0234375\n",
      "[step: 466] loss: 89759.109375\n",
      "[step: 467] loss: 89749.59375\n",
      "[step: 468] loss: 89742.203125\n",
      "[step: 469] loss: 89738.4140625\n",
      "[step: 470] loss: 89737.3359375\n",
      "[step: 471] loss: 89735.1171875\n",
      "[step: 472] loss: 89742.0546875\n",
      "[step: 473] loss: 89786.8046875\n",
      "[step: 474] loss: 89940.9453125\n",
      "[step: 475] loss: 90368.2421875\n",
      "[step: 476] loss: 90496.5859375\n",
      "[step: 477] loss: 90482.5703125\n",
      "[step: 478] loss: 89849.3125\n",
      "[step: 479] loss: 89670.78125\n",
      "[step: 480] loss: 89880.953125\n",
      "[step: 481] loss: 90013.0703125\n",
      "[step: 482] loss: 89961.171875\n",
      "[step: 483] loss: 89671.3984375\n",
      "[step: 484] loss: 89730.34375\n",
      "[step: 485] loss: 89959.515625\n",
      "[step: 486] loss: 89820.7578125\n",
      "[step: 487] loss: 89660.6875\n",
      "[step: 488] loss: 89619.578125\n",
      "[step: 489] loss: 89700.0390625\n",
      "[step: 490] loss: 89781.65625\n",
      "[step: 491] loss: 89701.3359375\n",
      "[step: 492] loss: 89615.4375\n",
      "[step: 493] loss: 89579.984375\n",
      "[step: 494] loss: 89596.015625\n",
      "[step: 495] loss: 89653.6171875\n",
      "[step: 496] loss: 89705.7578125\n",
      "[step: 497] loss: 89716.296875\n",
      "[step: 498] loss: 89640.4453125\n",
      "[step: 499] loss: 89566.109375\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _, step_loss = final_sess.run([train, loss], feed_dict={\n",
    "                            X: finalX, Y: finalY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divide_size :  15000\n",
      "X_test shape :  (26240, 4)\n",
      "X_test_final shape :  (2624, 10, 4)\n",
      "predict shape :  (2624, 1)\n"
     ]
    }
   ],
   "source": [
    "#s_t = time.time()\n",
    "\n",
    "test_path = 'test'\n",
    "test_list = os.listdir(test_path)\n",
    "X_test = []\n",
    "number_of_csvs = 2624\n",
    "number_of_rows_in_each_csvs = 150000\n",
    "divide_size = int(number_of_rows_in_each_csvs/seq_length)\n",
    "print(\"divide_size : \", divide_size)\n",
    "\n",
    "\n",
    "for path in test_list:\n",
    "    test = pd.read_csv(os.path.join(test_path, path), dtype=np.float64)\n",
    "    #print(\"test shape : \", test.shape)\n",
    "    for i in range(0, seq_length):\n",
    "        #test_divided_by_size = test[i*size:(i+1)*15000, :]\n",
    "        test_divided_by_length = test[i*divide_size:(i+1)*divide_size]\n",
    "        #print(\"test_divided_by_length shape : \", test_divided_by_length.shape)\n",
    "        X_test.append(gen_features(test_divided_by_length).values)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "#X_test = X_test[:, [0,4,5,6,7,8,11,12]]\n",
    "#X_test = X_test[:, [0,4,5,6,7,8,11,12]]\n",
    "X_test = X_test[:, [0,1,2,3]]\n",
    "\n",
    "print(\"X_test shape : \", X_test.shape) #maybe 2624*10 = 26240 (26240,13)\n",
    "#print(X_test[0:1000, :])\n",
    "\n",
    "def build_dataset_for_predict(XX, seq_length):\n",
    "    dataX = []\n",
    "    for i in range(0, number_of_csvs*seq_length,seq_length): # 0 ~ 26230\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        dataX.append(_x)\n",
    "    return np.array(dataX)\n",
    "\n",
    "X_test_final = build_dataset_for_predict(X_test, seq_length)\n",
    "print(\"X_test_final shape : \", X_test_final.shape) # (26240, 10, 13)\n",
    "\n",
    "#X_test = MinMaxScaler(X_test)\n",
    "#scaler = StandardScaler().fit(X_test)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "predict = final_sess.run(Y_pred, feed_dict={X: X_test_final})\n",
    "print(\"predict shape : \", predict.shape)\n",
    "\n",
    "# reg = reg_grid.best_estimator_\n",
    "# reg.fit(X_train, y_train)\n",
    "# y_pred = reg.predict(X_test)\n",
    "pd.DataFrame(predict).to_csv('20190325_prediction_LSTM_5.csv', header=None)\n",
    "\n",
    "#e_t = time.time()\n",
    "#duration = (e_t - s_t) / 60\n",
    "#print(\"{:.1f} min tooked\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
