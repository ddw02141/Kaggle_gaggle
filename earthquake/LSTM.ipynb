{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    \n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def gen_features(X):\n",
    "    strain = []\n",
    "    strain.append(X.mean())\n",
    "    strain.append(X.std())\n",
    "    strain.append(X.min())\n",
    "    strain.append(X.max())\n",
    "    strain.append(X.kurtosis())\n",
    "    strain.append(X.skew())\n",
    "    strain.append(np.quantile(X,0.01))\n",
    "    strain.append(np.quantile(X,0.05))\n",
    "    strain.append(np.quantile(X,0.95))\n",
    "    strain.append(np.quantile(X,0.99))\n",
    "    strain.append(np.abs(X).max())\n",
    "    strain.append(np.abs(X).mean())\n",
    "    strain.append(np.abs(X).std())\n",
    "    return pd.Series(strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy shape :  (419431, 13) y shape :  (419431, 1)\n",
      "train_setx shape :  (293601, 13) train_sety shape :  (293601, 1)\n",
      "test_setx shape :  (125930, 13) test_sety shape :  (125930, 1)\n",
      "trainX shape :  (293501, 100, 13) trainY shape :  (293501, 1)\n",
      "testX shape :  (125830, 100, 13) testY shape :  (125830, 1)\n",
      "WARNING:tensorflow:From <ipython-input-3-18f5e3cb1d51>:71: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Y_pred shape :  (?, 1)\n",
      "[step: 0] loss: 12432017.0\n",
      "[step: 1] loss: 11858707.0\n",
      "[step: 2] loss: 10659521.0\n",
      "[step: 3] loss: 8797360.0\n",
      "[step: 4] loss: 7515975.0\n",
      "[step: 5] loss: 6917855.0\n",
      "[step: 6] loss: 6435944.5\n",
      "[step: 7] loss: 5946326.5\n",
      "[step: 8] loss: 5659695.5\n",
      "[step: 9] loss: 5530247.5\n",
      "[step: 10] loss: 5426175.0\n",
      "[step: 11] loss: 5327609.0\n",
      "[step: 12] loss: 5233472.0\n",
      "[step: 13] loss: 5143450.0\n",
      "[step: 14] loss: 5057409.0\n",
      "[step: 15] loss: 4975326.0\n",
      "[step: 16] loss: 4897199.0\n",
      "[step: 17] loss: 4823033.0\n",
      "[step: 18] loss: 4752809.5\n",
      "[step: 19] loss: 4686519.5\n",
      "[step: 20] loss: 4624135.0\n",
      "[step: 21] loss: 4565596.0\n",
      "[step: 22] loss: 4510850.5\n",
      "[step: 23] loss: 4459815.5\n",
      "[step: 24] loss: 4412406.0\n",
      "[step: 25] loss: 4368519.5\n",
      "[step: 26] loss: 4328038.5\n",
      "[step: 27] loss: 4290851.5\n",
      "[step: 28] loss: 4256806.0\n",
      "[step: 29] loss: 4225750.5\n",
      "[step: 30] loss: 4197494.5\n",
      "[step: 31] loss: 4171766.5\n",
      "[step: 32] loss: 4148212.0\n",
      "[step: 33] loss: 4126287.5\n",
      "[step: 34] loss: 4105123.25\n",
      "[step: 35] loss: 4083321.25\n",
      "[step: 36] loss: 4059112.75\n",
      "[step: 37] loss: 4031586.5\n",
      "[step: 38] loss: 4003495.25\n",
      "[step: 39] loss: 3982420.75\n",
      "[step: 40] loss: 3973186.0\n",
      "[step: 41] loss: 3969724.5\n",
      "[step: 42] loss: 3967885.0\n",
      "[step: 43] loss: 3971592.0\n",
      "[step: 44] loss: 3972431.0\n",
      "[step: 45] loss: 3966635.0\n",
      "[step: 46] loss: 3956966.5\n",
      "[step: 47] loss: 3946379.0\n",
      "[step: 48] loss: 3936704.25\n",
      "[step: 49] loss: 3928118.0\n",
      "[step: 50] loss: 3921258.0\n",
      "[step: 51] loss: 3915337.0\n",
      "[step: 52] loss: 3905745.0\n",
      "[step: 53] loss: 3890066.25\n",
      "[step: 54] loss: 3875003.0\n",
      "[step: 55] loss: 3866742.75\n",
      "[step: 56] loss: 3861054.75\n",
      "[step: 57] loss: 3846922.5\n",
      "[step: 58] loss: 3830520.75\n",
      "[step: 59] loss: 3821296.5\n",
      "[step: 60] loss: 3809686.25\n",
      "[step: 61] loss: 3807295.75\n",
      "[step: 62] loss: 3807440.5\n",
      "[step: 63] loss: 3800538.5\n",
      "[step: 64] loss: 3789464.0\n",
      "[step: 65] loss: 3784654.5\n",
      "[step: 66] loss: 3766487.25\n",
      "[step: 67] loss: 3764823.0\n",
      "[step: 68] loss: 3745638.75\n",
      "[step: 69] loss: 3746414.75\n",
      "[step: 70] loss: 3729624.0\n",
      "[step: 71] loss: 3728596.75\n",
      "[step: 72] loss: 3716686.75\n",
      "[step: 73] loss: 3710035.0\n",
      "[step: 74] loss: 3701140.75\n",
      "[step: 75] loss: 3689506.5\n",
      "[step: 76] loss: 3681772.25\n",
      "[step: 77] loss: 3667129.5\n",
      "[step: 78] loss: 3661567.5\n",
      "[step: 79] loss: 3649391.75\n",
      "[step: 80] loss: 3644768.5\n",
      "[step: 81] loss: 3632702.75\n",
      "[step: 82] loss: 3624836.25\n",
      "[step: 83] loss: 3612941.5\n",
      "[step: 84] loss: 3606214.0\n",
      "[step: 85] loss: 3596628.5\n"
     ]
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 100\n",
    "data_dim = 13\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('preprocessed/X_train_1.5K.csv', delimiter=',', usecols=(1,2,3,4,5,6,7,8,9,10,11,12,13))\n",
    "y = np.loadtxt('preprocessed/y_train_1.5K.csv', delimiter = ',', usecols=(1))\n",
    "y = np.reshape(y, (len(y),1))\n",
    "#print(xy)\n",
    "#print(y)\n",
    "print(\"xy shape : \", xy.shape, \"y shape : \",y.shape)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_setx = xy[0:train_size]\n",
    "train_sety = y[0:train_size]\n",
    "test_setx = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "test_sety = y[train_size - seq_length:]\n",
    "\n",
    "# Scale each\n",
    "# train_setx = MinMaxScaler(train_setx)\n",
    "# train_sety = MinMaxScaler(train_sety)\n",
    "# test_setx = MinMaxScaler(test_setx)\n",
    "# test_sety = MinMaxScaler(test_sety)\n",
    "print(\"train_setx shape : \", train_setx.shape, \"train_sety shape : \", train_sety.shape)\n",
    "print(\"test_setx shape : \", test_setx.shape, \"test_sety shape : \", test_sety.shape)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(XX, YY, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(XX) - seq_length):\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        _y = YY[i+seq_length]\n",
    "\n",
    "        #print(\"_x shape : \", _x.shape)\n",
    "        #print(\"_y shape : \", _y.shape)\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_setx, train_sety, seq_length)\n",
    "testX, testY = build_dataset(test_setx, test_sety, seq_length)\n",
    "\n",
    "# print(\"trainX\")\n",
    "# print(trainX)\n",
    "# print(\"trainY\")\n",
    "# print(trainY)\n",
    "# print(\"testX\")\n",
    "# print(testX)\n",
    "# print(\"testY\")\n",
    "# print(testY)\n",
    "print(\"trainX shape : \", trainX.shape, \"trainY shape : \", trainY.shape)\n",
    "print(\"testX shape : \", testX.shape, \"testY shape : \", testY.shape)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "print(\"Y_pred shape : \", Y_pred.shape)\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "sess = tf.Session() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training step\n",
    "for i in range(iterations):\n",
    "    _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                            X: trainX, Y: trainY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        \n",
    "#     scores = cross_validate(reg_lin, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "#     print(scores['test_score'])\n",
    "#     print(\"{:.4f}\".format(sum(scores['test_score']) / 5))\n",
    "\n",
    "# Test step\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "print(\"test_predict shape : \", test_predict.shape)\n",
    "rmse_val = sess.run(rmse, feed_dict={\n",
    "                targets: testY, predictions: test_predict})\n",
    "print(\"RMSE: {}\".format(rmse_val))\n",
    "    \n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(testY)\n",
    "plt.plot(test_predict)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Time To Failure\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_t = time.time()\n",
    "\n",
    "test_path = 'test'\n",
    "test_list = os.listdir(test_path)\n",
    "X_test = []\n",
    "number_of_csvs = 2624\n",
    "number_of_rows_in_each_csvs = 150000\n",
    "divide_size = int(number_of_rows_in_each_csvs/seq_length)\n",
    "print(\"divide_size : \", divide_size)\n",
    "\n",
    "\n",
    "for path in test_list:\n",
    "    test = pd.read_csv(os.path.join(test_path, path), dtype=np.float64)\n",
    "    #print(\"test shape : \", test.shape)\n",
    "    for i in range(0, seq_length):\n",
    "        #test_divided_by_size = test[i*size:(i+1)*15000, :]\n",
    "        test_divided_by_length = test[i*divide_size:(i+1)*divide_size]\n",
    "        #print(\"test_divided_by_length shape : \", test_divided_by_length.shape)\n",
    "\n",
    "        X_test.append(gen_features(test_divided_by_length).values)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "print(\"X_test shape : \", X_test.shape) #maybe 2624*10 = 26240 (26240,13)\n",
    "#print(X_test[0:1000, :])\n",
    "\n",
    "def build_dataset_for_predict(XX, seq_length):\n",
    "    dataX = []\n",
    "    for i in range(0, number_of_csvs*seq_length,seq_length): # 0 ~ 26230\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        dataX.append(_x)\n",
    "    return np.array(dataX)\n",
    "\n",
    "X_test_final = build_dataset_for_predict(X_test, seq_length)\n",
    "print(\"X_test_final shape : \", X_test_final.shape) # (26240, 10, 13)\n",
    "\n",
    "#X_test = MinMaxScaler(X_test)\n",
    "#scaler = StandardScaler().fit(X_test)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "predict = sess.run(Y_pred, feed_dict={X: X_test_final})\n",
    "print(\"predict shape : \", predict.shape)\n",
    "\n",
    "# reg = reg_grid.best_estimator_\n",
    "# reg.fit(X_train, y_train)\n",
    "# y_pred = reg.predict(X_test)\n",
    "pd.DataFrame(predict).to_csv('prediction_LSTM.csv', header=None)\n",
    "\n",
    "#e_t = time.time()\n",
    "#duration = (e_t - s_t) / 60\n",
    "#print(\"{:.1f} min tooked\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
