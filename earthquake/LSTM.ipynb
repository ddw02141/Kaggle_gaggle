{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    \n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "def gen_features(X):\n",
    "    strain = []\n",
    "    strain.append(X.mean())\n",
    "    strain.append(X.std())\n",
    "    strain.append(X.min())\n",
    "    strain.append(X.max())\n",
    "    strain.append(X.kurtosis())\n",
    "    strain.append(X.skew())\n",
    "    strain.append(np.quantile(X,0.01))\n",
    "    strain.append(np.quantile(X,0.05))\n",
    "    strain.append(np.quantile(X,0.95))\n",
    "    strain.append(np.quantile(X,0.99))\n",
    "    strain.append(np.abs(X).max())\n",
    "    strain.append(np.abs(X).mean())\n",
    "    strain.append(np.abs(X).std())\n",
    "    return pd.Series(strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy shape :  (41943, 13) y shape :  (41943, 1)\n",
      "train_setx shape :  (29360, 13) train_sety shape :  (29360, 1)\n",
      "test_setx shape :  (12593, 13) test_sety shape :  (12593, 1)\n",
      "trainX shape :  (29350, 10, 13) trainY shape :  (29350, 1)\n",
      "testX shape :  (12583, 10, 13) testY shape :  (12583, 1)\n",
      "WARNING:tensorflow:From <ipython-input-3-1a8f5527db88>:71: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Y_pred shape :  (?, 1)\n",
      "[step: 0] loss: 6.004570484161377\n",
      "[step: 1] loss: 5.178064823150635\n",
      "[step: 2] loss: 4.799934387207031\n",
      "[step: 3] loss: 4.566197395324707\n",
      "[step: 4] loss: 4.398525714874268\n",
      "[step: 5] loss: 4.263012886047363\n",
      "[step: 6] loss: 4.135405540466309\n",
      "[step: 7] loss: 4.007088661193848\n",
      "[step: 8] loss: 3.874572277069092\n",
      "[step: 9] loss: 3.7521677017211914\n",
      "[step: 10] loss: 3.6601641178131104\n",
      "[step: 11] loss: 3.596587896347046\n",
      "[step: 12] loss: 3.5533547401428223\n",
      "[step: 13] loss: 3.521327257156372\n",
      "[step: 14] loss: 3.495805025100708\n",
      "[step: 15] loss: 3.475432872772217\n",
      "[step: 16] loss: 3.4584004878997803\n",
      "[step: 17] loss: 3.4436187744140625\n",
      "[step: 18] loss: 3.4300763607025146\n",
      "[step: 19] loss: 3.4168131351470947\n",
      "[step: 20] loss: 3.403061866760254\n",
      "[step: 21] loss: 3.3879401683807373\n",
      "[step: 22] loss: 3.3704566955566406\n",
      "[step: 23] loss: 3.3494198322296143\n",
      "[step: 24] loss: 3.3235621452331543\n",
      "[step: 25] loss: 3.2916197776794434\n",
      "[step: 26] loss: 3.253444194793701\n",
      "[step: 27] loss: 3.2106175422668457\n",
      "[step: 28] loss: 3.1686415672302246\n",
      "[step: 29] loss: 3.1369519233703613\n",
      "[step: 30] loss: 3.1269259452819824\n",
      "[step: 31] loss: 3.1323416233062744\n",
      "[step: 32] loss: 3.135699987411499\n",
      "[step: 33] loss: 3.133998155593872\n",
      "[step: 34] loss: 3.1289961338043213\n",
      "[step: 35] loss: 3.1215715408325195\n",
      "[step: 36] loss: 3.111480236053467\n",
      "[step: 37] loss: 3.0976760387420654\n",
      "[step: 38] loss: 3.0782530307769775\n",
      "[step: 39] loss: 3.0515294075012207\n",
      "[step: 40] loss: 3.0184435844421387\n",
      "[step: 41] loss: 2.984556198120117\n",
      "[step: 42] loss: 2.960282802581787\n",
      "[step: 43] loss: 2.9553966522216797\n",
      "[step: 44] loss: 2.958486795425415\n",
      "[step: 45] loss: 2.9466352462768555\n",
      "[step: 46] loss: 2.9205310344696045\n",
      "[step: 47] loss: 2.892282485961914\n",
      "[step: 48] loss: 2.8725736141204834\n",
      "[step: 49] loss: 2.8653838634490967\n",
      "[step: 50] loss: 2.8677403926849365\n",
      "[step: 51] loss: 2.8732964992523193\n",
      "[step: 52] loss: 2.876504421234131\n",
      "[step: 53] loss: 2.8743698596954346\n",
      "[step: 54] loss: 2.866572856903076\n",
      "[step: 55] loss: 2.8545753955841064\n",
      "[step: 56] loss: 2.8409347534179688\n",
      "[step: 57] loss: 2.828474998474121\n",
      "[step: 58] loss: 2.8193864822387695\n",
      "[step: 59] loss: 2.814523220062256\n",
      "[step: 60] loss: 2.8127493858337402\n",
      "[step: 61] loss: 2.8120291233062744\n",
      "[step: 62] loss: 2.8103458881378174\n",
      "[step: 63] loss: 2.8067684173583984\n",
      "[step: 64] loss: 2.801517963409424\n",
      "[step: 65] loss: 2.7955644130706787\n",
      "[step: 66] loss: 2.7899274826049805\n",
      "[step: 67] loss: 2.7852184772491455\n",
      "[step: 68] loss: 2.781451940536499\n",
      "[step: 69] loss: 2.7783432006835938\n",
      "[step: 70] loss: 2.7753915786743164\n",
      "[step: 71] loss: 2.7721590995788574\n",
      "[step: 72] loss: 2.768477439880371\n",
      "[step: 73] loss: 2.764488697052002\n",
      "[step: 74] loss: 2.760507822036743\n",
      "[step: 75] loss: 2.756929397583008\n",
      "[step: 76] loss: 2.753919839859009\n",
      "[step: 77] loss: 2.751424789428711\n",
      "[step: 78] loss: 2.7491841316223145\n",
      "[step: 79] loss: 2.746910572052002\n",
      "[step: 80] loss: 2.744321346282959\n",
      "[step: 81] loss: 2.741358757019043\n",
      "[step: 82] loss: 2.7382054328918457\n",
      "[step: 83] loss: 2.7351272106170654\n",
      "[step: 84] loss: 2.7323310375213623\n",
      "[step: 85] loss: 2.7298808097839355\n",
      "[step: 86] loss: 2.727696180343628\n",
      "[step: 87] loss: 2.725616216659546\n",
      "[step: 88] loss: 2.7235124111175537\n",
      "[step: 89] loss: 2.72129225730896\n",
      "[step: 90] loss: 2.7189748287200928\n",
      "[step: 91] loss: 2.716625690460205\n",
      "[step: 92] loss: 2.7143096923828125\n",
      "[step: 93] loss: 2.7120983600616455\n",
      "[step: 94] loss: 2.7099874019622803\n",
      "[step: 95] loss: 2.707930326461792\n",
      "[step: 96] loss: 2.7059051990509033\n",
      "[step: 97] loss: 2.703866958618164\n",
      "[step: 98] loss: 2.701801300048828\n",
      "[step: 99] loss: 2.6997361183166504\n",
      "[step: 100] loss: 2.697688341140747\n",
      "[step: 101] loss: 2.6957027912139893\n",
      "[step: 102] loss: 2.6937859058380127\n",
      "[step: 103] loss: 2.6919007301330566\n",
      "[step: 104] loss: 2.6900179386138916\n",
      "[step: 105] loss: 2.688121795654297\n",
      "[step: 106] loss: 2.686206102371216\n",
      "[step: 107] loss: 2.6842892169952393\n",
      "[step: 108] loss: 2.6823644638061523\n",
      "[step: 109] loss: 2.680457592010498\n",
      "[step: 110] loss: 2.678542137145996\n",
      "[step: 111] loss: 2.676654577255249\n",
      "[step: 112] loss: 2.674797773361206\n",
      "[step: 113] loss: 2.6729369163513184\n",
      "[step: 114] loss: 2.6710736751556396\n",
      "[step: 115] loss: 2.6691956520080566\n",
      "[step: 116] loss: 2.667327404022217\n",
      "[step: 117] loss: 2.665470600128174\n",
      "[step: 118] loss: 2.663628578186035\n",
      "[step: 119] loss: 2.6617934703826904\n",
      "[step: 120] loss: 2.659954786300659\n",
      "[step: 121] loss: 2.6581170558929443\n",
      "[step: 122] loss: 2.6562952995300293\n",
      "[step: 123] loss: 2.6545207500457764\n",
      "[step: 124] loss: 2.6527438163757324\n",
      "[step: 125] loss: 2.6509287357330322\n",
      "[step: 126] loss: 2.649122953414917\n",
      "[step: 127] loss: 2.6473007202148438\n",
      "[step: 128] loss: 2.6454436779022217\n",
      "[step: 129] loss: 2.643568754196167\n",
      "[step: 130] loss: 2.6416759490966797\n",
      "[step: 131] loss: 2.639770030975342\n",
      "[step: 132] loss: 2.637864589691162\n",
      "[step: 133] loss: 2.6359570026397705\n",
      "[step: 134] loss: 2.6340298652648926\n",
      "[step: 135] loss: 2.6321208477020264\n",
      "[step: 136] loss: 2.630263566970825\n",
      "[step: 137] loss: 2.628368616104126\n",
      "[step: 138] loss: 2.6263866424560547\n",
      "[step: 139] loss: 2.6243648529052734\n",
      "[step: 140] loss: 2.622335433959961\n",
      "[step: 141] loss: 2.6202640533447266\n",
      "[step: 142] loss: 2.6181375980377197\n",
      "[step: 143] loss: 2.615967035293579\n",
      "[step: 144] loss: 2.6137990951538086\n",
      "[step: 145] loss: 2.6117026805877686\n",
      "[step: 146] loss: 2.609767198562622\n",
      "[step: 147] loss: 2.607984781265259\n",
      "[step: 148] loss: 2.6062984466552734\n",
      "[step: 149] loss: 2.6047892570495605\n",
      "[step: 150] loss: 2.6033694744110107\n",
      "[step: 151] loss: 2.601912498474121\n",
      "[step: 152] loss: 2.6004042625427246\n",
      "[step: 153] loss: 2.5988941192626953\n",
      "[step: 154] loss: 2.5974481105804443\n",
      "[step: 155] loss: 2.5959973335266113\n",
      "[step: 156] loss: 2.5944759845733643\n",
      "[step: 157] loss: 2.5929112434387207\n",
      "[step: 158] loss: 2.5913569927215576\n",
      "[step: 159] loss: 2.5898146629333496\n",
      "[step: 160] loss: 2.588243007659912\n",
      "[step: 161] loss: 2.5866293907165527\n",
      "[step: 162] loss: 2.5850062370300293\n",
      "[step: 163] loss: 2.5834121704101562\n",
      "[step: 164] loss: 2.581827163696289\n",
      "[step: 165] loss: 2.580220937728882\n",
      "[step: 166] loss: 2.5786068439483643\n",
      "[step: 167] loss: 2.5770111083984375\n",
      "[step: 168] loss: 2.575413703918457\n",
      "[step: 169] loss: 2.573784828186035\n",
      "[step: 170] loss: 2.5720975399017334\n",
      "[step: 171] loss: 2.5703353881835938\n",
      "[step: 172] loss: 2.56843638420105\n",
      "[step: 173] loss: 2.5662691593170166\n",
      "[step: 174] loss: 2.5636932849884033\n",
      "[step: 175] loss: 2.56058669090271\n",
      "[step: 176] loss: 2.5572092533111572\n",
      "[step: 177] loss: 2.555649518966675\n",
      "[step: 178] loss: 2.5538244247436523\n",
      "[step: 179] loss: 2.5498054027557373\n",
      "[step: 180] loss: 2.546494722366333\n",
      "[step: 181] loss: 2.5430684089660645\n",
      "[step: 182] loss: 2.538558006286621\n",
      "[step: 183] loss: 2.5374085903167725\n",
      "[step: 184] loss: 2.5339837074279785\n",
      "[step: 185] loss: 2.5312695503234863\n",
      "[step: 186] loss: 2.529205799102783\n",
      "[step: 187] loss: 2.525353193283081\n",
      "[step: 188] loss: 2.5208373069763184\n",
      "[step: 189] loss: 2.5178534984588623\n",
      "[step: 190] loss: 2.513584613800049\n",
      "[step: 191] loss: 2.5110933780670166\n",
      "[step: 192] loss: 2.50854754447937\n",
      "[step: 193] loss: 2.505089282989502\n",
      "[step: 194] loss: 2.5016016960144043\n",
      "[step: 195] loss: 2.4979023933410645\n",
      "[step: 196] loss: 2.494495391845703\n",
      "[step: 197] loss: 2.491959810256958\n",
      "[step: 198] loss: 2.489044427871704\n",
      "[step: 199] loss: 2.4860918521881104\n",
      "[step: 200] loss: 2.482703447341919\n",
      "[step: 201] loss: 2.479520320892334\n",
      "[step: 202] loss: 2.476628541946411\n",
      "[step: 203] loss: 2.4737119674682617\n",
      "[step: 204] loss: 2.471116542816162\n",
      "[step: 205] loss: 2.4681835174560547\n",
      "[step: 206] loss: 2.4653513431549072\n",
      "[step: 207] loss: 2.4624578952789307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 208] loss: 2.4596428871154785\n",
      "[step: 209] loss: 2.4570531845092773\n",
      "[step: 210] loss: 2.4543843269348145\n",
      "[step: 211] loss: 2.4518446922302246\n",
      "[step: 212] loss: 2.449127674102783\n",
      "[step: 213] loss: 2.4465625286102295\n",
      "[step: 214] loss: 2.443917989730835\n",
      "[step: 215] loss: 2.4414632320404053\n",
      "[step: 216] loss: 2.438997268676758\n",
      "[step: 217] loss: 2.4365828037261963\n",
      "[step: 218] loss: 2.434112310409546\n",
      "[step: 219] loss: 2.4316630363464355\n",
      "[step: 220] loss: 2.4292938709259033\n",
      "[step: 221] loss: 2.426922082901001\n",
      "[step: 222] loss: 2.4246180057525635\n",
      "[step: 223] loss: 2.422238826751709\n",
      "[step: 224] loss: 2.4199488162994385\n",
      "[step: 225] loss: 2.417630672454834\n",
      "[step: 226] loss: 2.415421485900879\n",
      "[step: 227] loss: 2.4131481647491455\n",
      "[step: 228] loss: 2.4109442234039307\n",
      "[step: 229] loss: 2.4087045192718506\n",
      "[step: 230] loss: 2.406526803970337\n",
      "[step: 231] loss: 2.4043772220611572\n",
      "[step: 232] loss: 2.4022562503814697\n",
      "[step: 233] loss: 2.4001383781433105\n",
      "[step: 234] loss: 2.39802622795105\n",
      "[step: 235] loss: 2.3959665298461914\n",
      "[step: 236] loss: 2.3939082622528076\n",
      "[step: 237] loss: 2.391887664794922\n",
      "[step: 238] loss: 2.3898751735687256\n",
      "[step: 239] loss: 2.3878674507141113\n",
      "[step: 240] loss: 2.3858988285064697\n",
      "[step: 241] loss: 2.3839309215545654\n",
      "[step: 242] loss: 2.3819873332977295\n",
      "[step: 243] loss: 2.380059242248535\n",
      "[step: 244] loss: 2.3781275749206543\n",
      "[step: 245] loss: 2.3762311935424805\n",
      "[step: 246] loss: 2.374333143234253\n",
      "[step: 247] loss: 2.3724465370178223\n",
      "[step: 248] loss: 2.3705594539642334\n",
      "[step: 249] loss: 2.368678092956543\n",
      "[step: 250] loss: 2.366816997528076\n",
      "[step: 251] loss: 2.3649654388427734\n",
      "[step: 252] loss: 2.3631198406219482\n",
      "[step: 253] loss: 2.3612864017486572\n",
      "[step: 254] loss: 2.3594603538513184\n",
      "[step: 255] loss: 2.357630968093872\n",
      "[step: 256] loss: 2.355799436569214\n",
      "[step: 257] loss: 2.3539788722991943\n",
      "[step: 258] loss: 2.3522186279296875\n",
      "[step: 259] loss: 2.3505208492279053\n",
      "[step: 260] loss: 2.348853349685669\n",
      "[step: 261] loss: 2.3471269607543945\n",
      "[step: 262] loss: 2.345358371734619\n",
      "[step: 263] loss: 2.343564033508301\n",
      "[step: 264] loss: 2.3418078422546387\n",
      "[step: 265] loss: 2.340132474899292\n",
      "[step: 266] loss: 2.3385307788848877\n",
      "[step: 267] loss: 2.3371458053588867\n",
      "[step: 268] loss: 2.3355839252471924\n",
      "[step: 269] loss: 2.3337833881378174\n",
      "[step: 270] loss: 2.3318347930908203\n",
      "[step: 271] loss: 2.3302173614501953\n",
      "[step: 272] loss: 2.3289072513580322\n",
      "[step: 273] loss: 2.327512741088867\n",
      "[step: 274] loss: 2.3259670734405518\n",
      "[step: 275] loss: 2.324141263961792\n",
      "[step: 276] loss: 2.3225529193878174\n",
      "[step: 277] loss: 2.32125186920166\n",
      "[step: 278] loss: 2.319789171218872\n",
      "[step: 279] loss: 2.3180651664733887\n",
      "[step: 280] loss: 2.3164150714874268\n",
      "[step: 281] loss: 2.3149659633636475\n",
      "[step: 282] loss: 2.3135952949523926\n",
      "[step: 283] loss: 2.312131404876709\n",
      "[step: 284] loss: 2.3106064796447754\n",
      "[step: 285] loss: 2.3091495037078857\n",
      "[step: 286] loss: 2.307826042175293\n",
      "[step: 287] loss: 2.3065178394317627\n",
      "[step: 288] loss: 2.304988384246826\n",
      "[step: 289] loss: 2.3033525943756104\n",
      "[step: 290] loss: 2.3018991947174072\n",
      "[step: 291] loss: 2.3005802631378174\n",
      "[step: 292] loss: 2.2993545532226562\n",
      "[step: 293] loss: 2.2979958057403564\n",
      "[step: 294] loss: 2.2965123653411865\n",
      "[step: 295] loss: 2.295081377029419\n",
      "[step: 296] loss: 2.293858766555786\n",
      "[step: 297] loss: 2.2929015159606934\n",
      "[step: 298] loss: 2.291760206222534\n",
      "[step: 299] loss: 2.2903759479522705\n",
      "[step: 300] loss: 2.288853645324707\n",
      "[step: 301] loss: 2.287562131881714\n",
      "[step: 302] loss: 2.2864294052124023\n",
      "[step: 303] loss: 2.28513240814209\n",
      "[step: 304] loss: 2.2836313247680664\n",
      "[step: 305] loss: 2.2821826934814453\n",
      "[step: 306] loss: 2.280829668045044\n",
      "[step: 307] loss: 2.279388427734375\n",
      "[step: 308] loss: 2.277881622314453\n",
      "[step: 309] loss: 2.276421070098877\n",
      "[step: 310] loss: 2.2750244140625\n",
      "[step: 311] loss: 2.273650884628296\n",
      "[step: 312] loss: 2.2722396850585938\n",
      "[step: 313] loss: 2.27083683013916\n",
      "[step: 314] loss: 2.2694895267486572\n",
      "[step: 315] loss: 2.2681734561920166\n",
      "[step: 316] loss: 2.266724109649658\n",
      "[step: 317] loss: 2.2652781009674072\n",
      "[step: 318] loss: 2.263845443725586\n",
      "[step: 319] loss: 2.262432813644409\n",
      "[step: 320] loss: 2.2610838413238525\n",
      "[step: 321] loss: 2.259647846221924\n",
      "[step: 322] loss: 2.2582857608795166\n",
      "[step: 323] loss: 2.256953477859497\n",
      "[step: 324] loss: 2.2556772232055664\n",
      "[step: 325] loss: 2.2544586658477783\n",
      "[step: 326] loss: 2.2532169818878174\n",
      "[step: 327] loss: 2.2520806789398193\n",
      "[step: 328] loss: 2.2509965896606445\n",
      "[step: 329] loss: 2.2502565383911133\n",
      "[step: 330] loss: 2.249419927597046\n",
      "[step: 331] loss: 2.2488815784454346\n",
      "[step: 332] loss: 2.2476253509521484\n",
      "[step: 333] loss: 2.2461068630218506\n",
      "[step: 334] loss: 2.2448158264160156\n",
      "[step: 335] loss: 2.244274377822876\n",
      "[step: 336] loss: 2.2439470291137695\n",
      "[step: 337] loss: 2.242823362350464\n",
      "[step: 338] loss: 2.241161584854126\n",
      "[step: 339] loss: 2.240267276763916\n",
      "[step: 340] loss: 2.2398135662078857\n",
      "[step: 341] loss: 2.2387232780456543\n",
      "[step: 342] loss: 2.2374703884124756\n",
      "[step: 343] loss: 2.2366456985473633\n",
      "[step: 344] loss: 2.2360503673553467\n",
      "[step: 345] loss: 2.2351925373077393\n",
      "[step: 346] loss: 2.234093189239502\n",
      "[step: 347] loss: 2.233029365539551\n",
      "[step: 348] loss: 2.232278823852539\n",
      "[step: 349] loss: 2.2316689491271973\n",
      "[step: 350] loss: 2.2308692932128906\n",
      "[step: 351] loss: 2.22979998588562\n",
      "[step: 352] loss: 2.2288990020751953\n",
      "[step: 353] loss: 2.2283363342285156\n",
      "[step: 354] loss: 2.227529764175415\n",
      "[step: 355] loss: 2.22647762298584\n",
      "[step: 356] loss: 2.225554943084717\n",
      "[step: 357] loss: 2.224848985671997\n",
      "[step: 358] loss: 2.2241175174713135\n",
      "[step: 359] loss: 2.222996950149536\n",
      "[step: 360] loss: 2.2221946716308594\n",
      "[step: 361] loss: 2.2215416431427\n",
      "[step: 362] loss: 2.2204833030700684\n",
      "[step: 363] loss: 2.219470739364624\n",
      "[step: 364] loss: 2.2186124324798584\n",
      "[step: 365] loss: 2.217635154724121\n",
      "[step: 366] loss: 2.216697931289673\n",
      "[step: 367] loss: 2.215846300125122\n",
      "[step: 368] loss: 2.215003252029419\n",
      "[step: 369] loss: 2.2141542434692383\n",
      "[step: 370] loss: 2.2133142948150635\n",
      "[step: 371] loss: 2.212538003921509\n",
      "[step: 372] loss: 2.211854934692383\n",
      "[step: 373] loss: 2.21108078956604\n",
      "[step: 374] loss: 2.2102572917938232\n",
      "[step: 375] loss: 2.209488868713379\n",
      "[step: 376] loss: 2.208742618560791\n",
      "[step: 377] loss: 2.2080061435699463\n",
      "[step: 378] loss: 2.2072625160217285\n",
      "[step: 379] loss: 2.2064735889434814\n",
      "[step: 380] loss: 2.2056663036346436\n",
      "[step: 381] loss: 2.2049615383148193\n",
      "[step: 382] loss: 2.204400062561035\n",
      "[step: 383] loss: 2.203855276107788\n",
      "[step: 384] loss: 2.2033004760742188\n",
      "[step: 385] loss: 2.2027575969696045\n",
      "[step: 386] loss: 2.2021002769470215\n",
      "[step: 387] loss: 2.201169013977051\n",
      "[step: 388] loss: 2.2001025676727295\n",
      "[step: 389] loss: 2.1990489959716797\n",
      "[step: 390] loss: 2.198085308074951\n",
      "[step: 391] loss: 2.1974527835845947\n",
      "[step: 392] loss: 2.1972007751464844\n",
      "[step: 393] loss: 2.196967124938965\n",
      "[step: 394] loss: 2.1963534355163574\n",
      "[step: 395] loss: 2.1953136920928955\n",
      "[step: 396] loss: 2.1940619945526123\n",
      "[step: 397] loss: 2.1930465698242188\n",
      "[step: 398] loss: 2.1927075386047363\n",
      "[step: 399] loss: 2.192955493927002\n",
      "[step: 400] loss: 2.1925902366638184\n",
      "[step: 401] loss: 2.191240072250366\n",
      "[step: 402] loss: 2.1895692348480225\n",
      "[step: 403] loss: 2.188753843307495\n",
      "[step: 404] loss: 2.1885221004486084\n",
      "[step: 405] loss: 2.188274383544922\n",
      "[step: 406] loss: 2.1872494220733643\n",
      "[step: 407] loss: 2.1859591007232666\n",
      "[step: 408] loss: 2.1850745677948\n",
      "[step: 409] loss: 2.184605598449707\n",
      "[step: 410] loss: 2.184047222137451\n",
      "[step: 411] loss: 2.1833879947662354\n",
      "[step: 412] loss: 2.1825127601623535\n",
      "[step: 413] loss: 2.181654214859009\n",
      "[step: 414] loss: 2.180995464324951\n",
      "[step: 415] loss: 2.1804752349853516\n",
      "[step: 416] loss: 2.179943084716797\n",
      "[step: 417] loss: 2.1791439056396484\n",
      "[step: 418] loss: 2.178408145904541\n",
      "[step: 419] loss: 2.177732229232788\n",
      "[step: 420] loss: 2.1770973205566406\n",
      "[step: 421] loss: 2.1765196323394775\n",
      "[step: 422] loss: 2.17588210105896\n",
      "[step: 423] loss: 2.1752262115478516\n",
      "[step: 424] loss: 2.1746935844421387\n",
      "[step: 425] loss: 2.1743338108062744\n",
      "[step: 426] loss: 2.1741886138916016\n",
      "[step: 427] loss: 2.175278663635254\n",
      "[step: 428] loss: 2.1765053272247314\n",
      "[step: 429] loss: 2.175222158432007\n",
      "[step: 430] loss: 2.172187328338623\n",
      "[step: 431] loss: 2.170973539352417\n",
      "[step: 432] loss: 2.170607805252075\n",
      "[step: 433] loss: 2.170682907104492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 434] loss: 2.1708908081054688\n",
      "[step: 435] loss: 2.1699211597442627\n",
      "[step: 436] loss: 2.1689181327819824\n",
      "[step: 437] loss: 2.1680657863616943\n",
      "[step: 438] loss: 2.168020725250244\n",
      "[step: 439] loss: 2.168231248855591\n",
      "[step: 440] loss: 2.1677639484405518\n",
      "[step: 441] loss: 2.167003631591797\n",
      "[step: 442] loss: 2.165860652923584\n",
      "[step: 443] loss: 2.1656558513641357\n",
      "[step: 444] loss: 2.166231870651245\n",
      "[step: 445] loss: 2.1662938594818115\n",
      "[step: 446] loss: 2.1655845642089844\n",
      "[step: 447] loss: 2.164027214050293\n",
      "[step: 448] loss: 2.1632697582244873\n",
      "[step: 449] loss: 2.163966178894043\n",
      "[step: 450] loss: 2.164266586303711\n",
      "[step: 451] loss: 2.1637332439422607\n",
      "[step: 452] loss: 2.1619503498077393\n",
      "[step: 453] loss: 2.161315679550171\n",
      "[step: 454] loss: 2.1623849868774414\n",
      "[step: 455] loss: 2.1625404357910156\n",
      "[step: 456] loss: 2.161184310913086\n",
      "[step: 457] loss: 2.1596310138702393\n",
      "[step: 458] loss: 2.159785270690918\n",
      "[step: 459] loss: 2.160294532775879\n",
      "[step: 460] loss: 2.1591062545776367\n",
      "[step: 461] loss: 2.1578166484832764\n",
      "[step: 462] loss: 2.1579699516296387\n",
      "[step: 463] loss: 2.1582071781158447\n",
      "[step: 464] loss: 2.1572742462158203\n",
      "[step: 465] loss: 2.15625262260437\n",
      "[step: 466] loss: 2.1562578678131104\n",
      "[step: 467] loss: 2.156514883041382\n",
      "[step: 468] loss: 2.1559650897979736\n",
      "[step: 469] loss: 2.1549928188323975\n",
      "[step: 470] loss: 2.154397964477539\n",
      "[step: 471] loss: 2.154595375061035\n",
      "[step: 472] loss: 2.155061960220337\n",
      "[step: 473] loss: 2.1541240215301514\n",
      "[step: 474] loss: 2.153043508529663\n",
      "[step: 475] loss: 2.1526598930358887\n",
      "[step: 476] loss: 2.1526968479156494\n",
      "[step: 477] loss: 2.152534008026123\n",
      "[step: 478] loss: 2.1517348289489746\n",
      "[step: 479] loss: 2.1509859561920166\n",
      "[step: 480] loss: 2.150623083114624\n",
      "[step: 481] loss: 2.1505181789398193\n",
      "[step: 482] loss: 2.150174140930176\n",
      "[step: 483] loss: 2.149526596069336\n",
      "[step: 484] loss: 2.148963689804077\n",
      "[step: 485] loss: 2.1485445499420166\n",
      "[step: 486] loss: 2.1483912467956543\n",
      "[step: 487] loss: 2.1481337547302246\n",
      "[step: 488] loss: 2.1479084491729736\n",
      "[step: 489] loss: 2.147594928741455\n",
      "[step: 490] loss: 2.1473498344421387\n",
      "[step: 491] loss: 2.147250175476074\n",
      "[step: 492] loss: 2.1471290588378906\n",
      "[step: 493] loss: 2.1467044353485107\n",
      "[step: 494] loss: 2.146090507507324\n",
      "[step: 495] loss: 2.1458892822265625\n",
      "[step: 496] loss: 2.145857334136963\n",
      "[step: 497] loss: 2.1460025310516357\n",
      "[step: 498] loss: 2.145504951477051\n",
      "[step: 499] loss: 2.1452434062957764\n",
      "test_predict shape :  (12583, 1)\n",
      "RMAE: 2.124746799468994\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 10\n",
    "data_dim = 13\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.005\n",
    "iterations = 500\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('preprocessed/X_train_15K.csv', delimiter=',', usecols=(1,2,3,4,5,6,7,8,9,10,11,12,13))\n",
    "y = np.loadtxt('preprocessed/y_train_15K.csv', delimiter = ',', usecols=(1))\n",
    "y = np.reshape(y, (len(y),1))\n",
    "#print(xy)\n",
    "#print(y)\n",
    "print(\"xy shape : \", xy.shape, \"y shape : \",y.shape)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_setx = xy[0:train_size]\n",
    "train_sety = y[0:train_size]\n",
    "test_setx = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "test_sety = y[train_size - seq_length:]\n",
    "\n",
    "# Scale each\n",
    "# train_setx = MinMaxScaler(train_setx)\n",
    "# train_sety = MinMaxScaler(train_sety)\n",
    "# test_setx = MinMaxScaler(test_setx)\n",
    "# test_sety = MinMaxScaler(test_sety)\n",
    "print(\"train_setx shape : \", train_setx.shape, \"train_sety shape : \", train_sety.shape)\n",
    "print(\"test_setx shape : \", test_setx.shape, \"test_sety shape : \", test_sety.shape)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(XX, YY, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(XX) - seq_length):\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        _y = YY[i+seq_length]\n",
    "\n",
    "        #print(\"_x shape : \", _x.shape)\n",
    "        #print(\"_y shape : \", _y.shape)\n",
    "        #print(_x, \"->\", _y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_setx, train_sety, seq_length)\n",
    "testX, testY = build_dataset(test_setx, test_sety, seq_length)\n",
    "finalX, finalY = build_dataset(xy, y, seq_length)\n",
    "# print(\"trainX\")\n",
    "# print(trainX)\n",
    "# print(\"trainY\")\n",
    "# print(trainY)\n",
    "# print(\"testX\")\n",
    "# print(testX)\n",
    "# print(\"testY\")\n",
    "# print(testY)\n",
    "print(\"trainX shape : \", trainX.shape, \"trainY shape : \", trainY.shape)\n",
    "print(\"testX shape : \", testX.shape, \"testY shape : \", testY.shape)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "print(\"Y_pred shape : \", Y_pred.shape)\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.abs(Y_pred - Y))  # sum of the absolute values 이거 reduced_mean으로 바꿔봤는데 오차 증가함\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "rmae = tf.reduce_mean(tf.abs(targets - predictions))\n",
    "\n",
    "sess = tf.Session() \n",
    "final_sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "final_sess.run(init)\n",
    "\n",
    "# Training step\n",
    "for i in range(iterations):\n",
    "    _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                            X: trainX, Y: trainY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        \n",
    "#     scores = cross_validate(reg_lin, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "#     print(scores['test_score'])\n",
    "#     print(\"{:.4f}\".format(sum(scores['test_score']) / 5))\n",
    "\n",
    "# Test step\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "print(\"test_predict shape : \", test_predict.shape)\n",
    "# rmse_val = sess.run(rmse, feed_dict={\n",
    "#                 targets: testY, predictions: test_predict})\n",
    "# print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "rmae_val = sess.run(rmae, feed_dict={\n",
    "                targets: testY, predictions: test_predict})\n",
    "print(\"RMAE: {}\".format(rmae_val))\n",
    "\n",
    "\n",
    "    \n",
    "# Plot predictions\n",
    "plt.plot(testY)\n",
    "plt.plot(test_predict)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Time To Failure\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 5.194668292999268\n",
      "[step: 1] loss: 5.018608093261719\n",
      "[step: 2] loss: 4.825779438018799\n",
      "[step: 3] loss: 4.629554271697998\n",
      "[step: 4] loss: 4.441823482513428\n",
      "[step: 5] loss: 4.268527507781982\n",
      "[step: 6] loss: 4.107725620269775\n",
      "[step: 7] loss: 3.957015037536621\n",
      "[step: 8] loss: 3.821275472640991\n",
      "[step: 9] loss: 3.7126171588897705\n",
      "[step: 10] loss: 3.6335296630859375\n",
      "[step: 11] loss: 3.5760741233825684\n",
      "[step: 12] loss: 3.5255019664764404\n",
      "[step: 13] loss: 3.4729673862457275\n",
      "[step: 14] loss: 3.41365385055542\n",
      "[step: 15] loss: 3.3476672172546387\n",
      "[step: 16] loss: 3.27473783493042\n",
      "[step: 17] loss: 3.19697642326355\n",
      "[step: 18] loss: 3.127798080444336\n",
      "[step: 19] loss: 3.104597568511963\n",
      "[step: 20] loss: 3.1184189319610596\n",
      "[step: 21] loss: 3.1239492893218994\n",
      "[step: 22] loss: 3.1223084926605225\n",
      "[step: 23] loss: 3.118065357208252\n",
      "[step: 24] loss: 3.1128478050231934\n",
      "[step: 25] loss: 3.1072418689727783\n",
      "[step: 26] loss: 3.1014928817749023\n",
      "[step: 27] loss: 3.0957255363464355\n",
      "[step: 28] loss: 3.0899736881256104\n",
      "[step: 29] loss: 3.0841925144195557\n",
      "[step: 30] loss: 3.078360080718994\n",
      "[step: 31] loss: 3.0724380016326904\n",
      "[step: 32] loss: 3.0663039684295654\n",
      "[step: 33] loss: 3.059814214706421\n",
      "[step: 34] loss: 3.052783966064453\n",
      "[step: 35] loss: 3.0448858737945557\n",
      "[step: 36] loss: 3.0356855392456055\n",
      "[step: 37] loss: 3.0246474742889404\n",
      "[step: 38] loss: 3.0109944343566895\n",
      "[step: 39] loss: 2.993807077407837\n",
      "[step: 40] loss: 2.972606658935547\n",
      "[step: 41] loss: 2.9480929374694824\n",
      "[step: 42] loss: 2.9223716259002686\n",
      "[step: 43] loss: 2.8985300064086914\n",
      "[step: 44] loss: 2.8791613578796387\n",
      "[step: 45] loss: 2.865647792816162\n",
      "[step: 46] loss: 2.85823655128479\n",
      "[step: 47] loss: 2.856123447418213\n",
      "[step: 48] loss: 2.8577516078948975\n",
      "[step: 49] loss: 2.860935688018799\n",
      "[step: 50] loss: 2.8635497093200684\n",
      "[step: 51] loss: 2.8639774322509766\n",
      "[step: 52] loss: 2.8615005016326904\n",
      "[step: 53] loss: 2.856180429458618\n",
      "[step: 54] loss: 2.84865403175354\n",
      "[step: 55] loss: 2.8397560119628906\n",
      "[step: 56] loss: 2.8305912017822266\n",
      "[step: 57] loss: 2.822075843811035\n",
      "[step: 58] loss: 2.8150100708007812\n",
      "[step: 59] loss: 2.809777021408081\n",
      "[step: 60] loss: 2.806293487548828\n",
      "[step: 61] loss: 2.8041322231292725\n",
      "[step: 62] loss: 2.8027138710021973\n",
      "[step: 63] loss: 2.801403284072876\n",
      "[step: 64] loss: 2.799926280975342\n",
      "[step: 65] loss: 2.7979345321655273\n",
      "[step: 66] loss: 2.79533052444458\n",
      "[step: 67] loss: 2.7921762466430664\n",
      "[step: 68] loss: 2.78861403465271\n",
      "[step: 69] loss: 2.78482985496521\n",
      "[step: 70] loss: 2.7810583114624023\n",
      "[step: 71] loss: 2.777480125427246\n",
      "[step: 72] loss: 2.774247646331787\n",
      "[step: 73] loss: 2.7714247703552246\n",
      "[step: 74] loss: 2.7690155506134033\n",
      "[step: 75] loss: 2.7669875621795654\n",
      "[step: 76] loss: 2.7651922702789307\n",
      "[step: 77] loss: 2.7634313106536865\n",
      "[step: 78] loss: 2.761630058288574\n",
      "[step: 79] loss: 2.7596938610076904\n",
      "[step: 80] loss: 2.7575838565826416\n",
      "[step: 81] loss: 2.7552990913391113\n",
      "[step: 82] loss: 2.75285005569458\n",
      "[step: 83] loss: 2.7504117488861084\n",
      "[step: 84] loss: 2.7480435371398926\n",
      "[step: 85] loss: 2.745762586593628\n",
      "[step: 86] loss: 2.7435858249664307\n",
      "[step: 87] loss: 2.7415027618408203\n",
      "[step: 88] loss: 2.7394790649414062\n",
      "[step: 89] loss: 2.7374565601348877\n",
      "[step: 90] loss: 2.735382080078125\n",
      "[step: 91] loss: 2.733210563659668\n",
      "[step: 92] loss: 2.7309000492095947\n",
      "[step: 93] loss: 2.7284300327301025\n",
      "[step: 94] loss: 2.725780725479126\n",
      "[step: 95] loss: 2.722951889038086\n",
      "[step: 96] loss: 2.719961643218994\n",
      "[step: 97] loss: 2.7168290615081787\n",
      "[step: 98] loss: 2.7135770320892334\n",
      "[step: 99] loss: 2.7102668285369873\n",
      "[step: 100] loss: 2.7069194316864014\n",
      "[step: 101] loss: 2.7035624980926514\n",
      "[step: 102] loss: 2.7002336978912354\n",
      "[step: 103] loss: 2.69695782661438\n",
      "[step: 104] loss: 2.6937873363494873\n",
      "[step: 105] loss: 2.690739631652832\n",
      "[step: 106] loss: 2.6878738403320312\n",
      "[step: 107] loss: 2.6852424144744873\n",
      "[step: 108] loss: 2.682828187942505\n",
      "[step: 109] loss: 2.6805922985076904\n",
      "[step: 110] loss: 2.6784842014312744\n",
      "[step: 111] loss: 2.676348924636841\n",
      "[step: 112] loss: 2.6740756034851074\n",
      "[step: 113] loss: 2.6715447902679443\n",
      "[step: 114] loss: 2.668689012527466\n",
      "[step: 115] loss: 2.6653919219970703\n",
      "[step: 116] loss: 2.661698341369629\n",
      "[step: 117] loss: 2.6576812267303467\n",
      "[step: 118] loss: 2.6534616947174072\n",
      "[step: 119] loss: 2.649127960205078\n",
      "[step: 120] loss: 2.644602060317993\n",
      "[step: 121] loss: 2.639733076095581\n",
      "[step: 122] loss: 2.634093761444092\n",
      "[step: 123] loss: 2.6271142959594727\n",
      "[step: 124] loss: 2.619415044784546\n",
      "[step: 125] loss: 2.612165927886963\n",
      "[step: 126] loss: 2.6048669815063477\n",
      "[step: 127] loss: 2.6008903980255127\n",
      "[step: 128] loss: 2.5987467765808105\n",
      "[step: 129] loss: 2.5966358184814453\n",
      "[step: 130] loss: 2.594026803970337\n",
      "[step: 131] loss: 2.5907132625579834\n",
      "[step: 132] loss: 2.5870299339294434\n",
      "[step: 133] loss: 2.5834333896636963\n",
      "[step: 134] loss: 2.5802767276763916\n",
      "[step: 135] loss: 2.577636241912842\n",
      "[step: 136] loss: 2.5752274990081787\n",
      "[step: 137] loss: 2.5727107524871826\n",
      "[step: 138] loss: 2.569885730743408\n",
      "[step: 139] loss: 2.5668234825134277\n",
      "[step: 140] loss: 2.5637683868408203\n",
      "[step: 141] loss: 2.5609395503997803\n",
      "[step: 142] loss: 2.5583643913269043\n",
      "[step: 143] loss: 2.5559377670288086\n",
      "[step: 144] loss: 2.5534861087799072\n",
      "[step: 145] loss: 2.5508687496185303\n",
      "[step: 146] loss: 2.5480923652648926\n",
      "[step: 147] loss: 2.5452985763549805\n",
      "[step: 148] loss: 2.542637348175049\n",
      "[step: 149] loss: 2.540144920349121\n",
      "[step: 150] loss: 2.5377628803253174\n",
      "[step: 151] loss: 2.5353634357452393\n",
      "[step: 152] loss: 2.532884359359741\n",
      "[step: 153] loss: 2.5303375720977783\n",
      "[step: 154] loss: 2.52781081199646\n",
      "[step: 155] loss: 2.5253655910491943\n",
      "[step: 156] loss: 2.5229997634887695\n",
      "[step: 157] loss: 2.5206685066223145\n",
      "[step: 158] loss: 2.518319845199585\n",
      "[step: 159] loss: 2.5159313678741455\n",
      "[step: 160] loss: 2.5135324001312256\n",
      "[step: 161] loss: 2.511164903640747\n",
      "[step: 162] loss: 2.5088632106781006\n",
      "[step: 163] loss: 2.5066046714782715\n",
      "[step: 164] loss: 2.5043511390686035\n",
      "[step: 165] loss: 2.5020804405212402\n",
      "[step: 166] loss: 2.4997994899749756\n",
      "[step: 167] loss: 2.4975414276123047\n",
      "[step: 168] loss: 2.495331287384033\n",
      "[step: 169] loss: 2.4931535720825195\n",
      "[step: 170] loss: 2.4909732341766357\n",
      "[step: 171] loss: 2.4887824058532715\n",
      "[step: 172] loss: 2.4865951538085938\n",
      "[step: 173] loss: 2.48443603515625\n",
      "[step: 174] loss: 2.4822988510131836\n",
      "[step: 175] loss: 2.4801712036132812\n",
      "[step: 176] loss: 2.478055238723755\n",
      "[step: 177] loss: 2.4759414196014404\n",
      "[step: 178] loss: 2.473850727081299\n",
      "[step: 179] loss: 2.4717659950256348\n",
      "[step: 180] loss: 2.469694137573242\n",
      "[step: 181] loss: 2.4676337242126465\n",
      "[step: 182] loss: 2.4655778408050537\n",
      "[step: 183] loss: 2.4635257720947266\n",
      "[step: 184] loss: 2.461505174636841\n",
      "[step: 185] loss: 2.4594950675964355\n",
      "[step: 186] loss: 2.4574906826019287\n",
      "[step: 187] loss: 2.4554765224456787\n",
      "[step: 188] loss: 2.4534544944763184\n",
      "[step: 189] loss: 2.4514331817626953\n",
      "[step: 190] loss: 2.449408769607544\n",
      "[step: 191] loss: 2.4473819732666016\n",
      "[step: 192] loss: 2.445343494415283\n",
      "[step: 193] loss: 2.4432973861694336\n",
      "[step: 194] loss: 2.441221237182617\n",
      "[step: 195] loss: 2.439128875732422\n",
      "[step: 196] loss: 2.4369993209838867\n",
      "[step: 197] loss: 2.4348294734954834\n",
      "[step: 198] loss: 2.4326117038726807\n",
      "[step: 199] loss: 2.4303464889526367\n",
      "[step: 200] loss: 2.4280219078063965\n",
      "[step: 201] loss: 2.4256772994995117\n",
      "[step: 202] loss: 2.423490285873413\n",
      "[step: 203] loss: 2.4209580421447754\n",
      "[step: 204] loss: 2.418341636657715\n",
      "[step: 205] loss: 2.415719747543335\n",
      "[step: 206] loss: 2.4131669998168945\n",
      "[step: 207] loss: 2.4102628231048584\n",
      "[step: 208] loss: 2.4066638946533203\n",
      "[step: 209] loss: 2.4028160572052\n",
      "[step: 210] loss: 2.3990535736083984\n",
      "[step: 211] loss: 2.3948681354522705\n",
      "[step: 212] loss: 2.3911728858947754\n",
      "[step: 213] loss: 2.387645721435547\n",
      "[step: 214] loss: 2.384997606277466\n",
      "[step: 215] loss: 2.3837242126464844\n",
      "[step: 216] loss: 2.3818795680999756\n",
      "[step: 217] loss: 2.3816349506378174\n",
      "[step: 218] loss: 2.3788883686065674\n",
      "[step: 219] loss: 2.3774988651275635\n",
      "[step: 220] loss: 2.3753628730773926\n",
      "[step: 221] loss: 2.3738667964935303\n",
      "[step: 222] loss: 2.3716251850128174\n",
      "[step: 223] loss: 2.3714144229888916\n",
      "[step: 224] loss: 2.3682477474212646\n",
      "[step: 225] loss: 2.3678243160247803\n",
      "[step: 226] loss: 2.36569881439209\n",
      "[step: 227] loss: 2.363563299179077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 228] loss: 2.362337827682495\n",
      "[step: 229] loss: 2.3599300384521484\n",
      "[step: 230] loss: 2.3578240871429443\n",
      "[step: 231] loss: 2.356865406036377\n",
      "[step: 232] loss: 2.354365348815918\n",
      "[step: 233] loss: 2.353508472442627\n",
      "[step: 234] loss: 2.3515162467956543\n",
      "[step: 235] loss: 2.3514058589935303\n",
      "[step: 236] loss: 2.349463701248169\n",
      "[step: 237] loss: 2.3477022647857666\n",
      "[step: 238] loss: 2.3460848331451416\n",
      "[step: 239] loss: 2.344414234161377\n",
      "[step: 240] loss: 2.342906951904297\n",
      "[step: 241] loss: 2.3416013717651367\n",
      "[step: 242] loss: 2.3397626876831055\n",
      "[step: 243] loss: 2.338709592819214\n",
      "[step: 244] loss: 2.3370778560638428\n",
      "[step: 245] loss: 2.3357231616973877\n",
      "[step: 246] loss: 2.3343777656555176\n",
      "[step: 247] loss: 2.3328726291656494\n",
      "[step: 248] loss: 2.3316290378570557\n",
      "[step: 249] loss: 2.3299922943115234\n",
      "[step: 250] loss: 2.328900098800659\n",
      "[step: 251] loss: 2.3274154663085938\n",
      "[step: 252] loss: 2.3263003826141357\n",
      "[step: 253] loss: 2.324916124343872\n",
      "[step: 254] loss: 2.3237595558166504\n",
      "[step: 255] loss: 2.322427272796631\n",
      "[step: 256] loss: 2.3212289810180664\n",
      "[step: 257] loss: 2.31990385055542\n",
      "[step: 258] loss: 2.3187367916107178\n",
      "[step: 259] loss: 2.3174755573272705\n",
      "[step: 260] loss: 2.3163156509399414\n",
      "[step: 261] loss: 2.315108060836792\n",
      "[step: 262] loss: 2.313931941986084\n",
      "[step: 263] loss: 2.3127541542053223\n",
      "[step: 264] loss: 2.3116111755371094\n",
      "[step: 265] loss: 2.310450315475464\n",
      "[step: 266] loss: 2.309337854385376\n",
      "[step: 267] loss: 2.308187961578369\n",
      "[step: 268] loss: 2.3070919513702393\n",
      "[step: 269] loss: 2.305960178375244\n",
      "[step: 270] loss: 2.304849624633789\n",
      "[step: 271] loss: 2.3037538528442383\n",
      "[step: 272] loss: 2.302652359008789\n",
      "[step: 273] loss: 2.301624059677124\n",
      "[step: 274] loss: 2.3004374504089355\n",
      "[step: 275] loss: 2.299330949783325\n",
      "[step: 276] loss: 2.298236846923828\n",
      "[step: 277] loss: 2.297152519226074\n",
      "[step: 278] loss: 2.2960386276245117\n",
      "[step: 279] loss: 2.2949347496032715\n",
      "[step: 280] loss: 2.293862819671631\n",
      "[step: 281] loss: 2.292816400527954\n",
      "[step: 282] loss: 2.291849136352539\n",
      "[step: 283] loss: 2.2910525798797607\n",
      "[step: 284] loss: 2.290038585662842\n",
      "[step: 285] loss: 2.288647413253784\n",
      "[step: 286] loss: 2.2879486083984375\n",
      "[step: 287] loss: 2.287614107131958\n",
      "[step: 288] loss: 2.2861366271972656\n",
      "[step: 289] loss: 2.2862260341644287\n",
      "[step: 290] loss: 2.285565137863159\n",
      "[step: 291] loss: 2.284153461456299\n",
      "[step: 292] loss: 2.2826406955718994\n",
      "[step: 293] loss: 2.2824831008911133\n",
      "[step: 294] loss: 2.2803544998168945\n",
      "[step: 295] loss: 2.2803564071655273\n",
      "[step: 296] loss: 2.2784688472747803\n",
      "[step: 297] loss: 2.278380870819092\n",
      "[step: 298] loss: 2.2772715091705322\n",
      "[step: 299] loss: 2.276766061782837\n",
      "[step: 300] loss: 2.2755544185638428\n",
      "[step: 301] loss: 2.275773525238037\n",
      "[step: 302] loss: 2.27386474609375\n",
      "[step: 303] loss: 2.2745471000671387\n",
      "[step: 304] loss: 2.2740535736083984\n",
      "[step: 305] loss: 2.273242950439453\n",
      "[step: 306] loss: 2.2721598148345947\n",
      "[step: 307] loss: 2.270939350128174\n",
      "[step: 308] loss: 2.2690165042877197\n",
      "[step: 309] loss: 2.269603967666626\n",
      "[step: 310] loss: 2.2678511142730713\n",
      "[step: 311] loss: 2.2667300701141357\n",
      "[step: 312] loss: 2.2665393352508545\n",
      "[step: 313] loss: 2.2654101848602295\n",
      "[step: 314] loss: 2.2641665935516357\n",
      "[step: 315] loss: 2.2633724212646484\n",
      "[step: 316] loss: 2.262953042984009\n",
      "[step: 317] loss: 2.261859178543091\n",
      "[step: 318] loss: 2.260525941848755\n",
      "[step: 319] loss: 2.261092185974121\n",
      "[step: 320] loss: 2.258958101272583\n",
      "[step: 321] loss: 2.259727954864502\n",
      "[step: 322] loss: 2.2576711177825928\n",
      "[step: 323] loss: 2.2565979957580566\n",
      "[step: 324] loss: 2.255460262298584\n",
      "[step: 325] loss: 2.254504680633545\n",
      "[step: 326] loss: 2.253791093826294\n",
      "[step: 327] loss: 2.2528233528137207\n",
      "[step: 328] loss: 2.251725435256958\n",
      "[step: 329] loss: 2.2509541511535645\n",
      "[step: 330] loss: 2.2502987384796143\n",
      "[step: 331] loss: 2.249333143234253\n",
      "[step: 332] loss: 2.2486891746520996\n",
      "[step: 333] loss: 2.247999668121338\n",
      "[step: 334] loss: 2.2473340034484863\n",
      "[step: 335] loss: 2.2467644214630127\n",
      "[step: 336] loss: 2.2459611892700195\n",
      "[step: 337] loss: 2.245337963104248\n",
      "[step: 338] loss: 2.2449584007263184\n",
      "[step: 339] loss: 2.244084596633911\n",
      "[step: 340] loss: 2.24338960647583\n",
      "[step: 341] loss: 2.242791175842285\n",
      "[step: 342] loss: 2.2421045303344727\n",
      "[step: 343] loss: 2.241504192352295\n",
      "[step: 344] loss: 2.2408864498138428\n",
      "[step: 345] loss: 2.2401890754699707\n",
      "[step: 346] loss: 2.2395763397216797\n",
      "[step: 347] loss: 2.2389097213745117\n",
      "[step: 348] loss: 2.2383313179016113\n",
      "[step: 349] loss: 2.2378296852111816\n",
      "[step: 350] loss: 2.236982583999634\n",
      "[step: 351] loss: 2.2363505363464355\n",
      "[step: 352] loss: 2.2357609272003174\n",
      "[step: 353] loss: 2.2351770401000977\n",
      "[step: 354] loss: 2.2344794273376465\n",
      "[step: 355] loss: 2.233940839767456\n",
      "[step: 356] loss: 2.233344078063965\n",
      "[step: 357] loss: 2.2326676845550537\n",
      "[step: 358] loss: 2.232135772705078\n",
      "[step: 359] loss: 2.2315356731414795\n",
      "[step: 360] loss: 2.230992078781128\n",
      "[step: 361] loss: 2.230412483215332\n",
      "[step: 362] loss: 2.2297732830047607\n",
      "[step: 363] loss: 2.2292323112487793\n",
      "[step: 364] loss: 2.2289679050445557\n",
      "[step: 365] loss: 2.2288827896118164\n",
      "[step: 366] loss: 2.2292752265930176\n",
      "[step: 367] loss: 2.2299208641052246\n",
      "[step: 368] loss: 2.227323532104492\n",
      "[step: 369] loss: 2.2257421016693115\n",
      "[step: 370] loss: 2.225994348526001\n",
      "[step: 371] loss: 2.2261767387390137\n",
      "[step: 372] loss: 2.2253143787384033\n",
      "[step: 373] loss: 2.223719835281372\n",
      "[step: 374] loss: 2.2243738174438477\n",
      "[step: 375] loss: 2.2250325679779053\n",
      "[step: 376] loss: 2.222480058670044\n",
      "[step: 377] loss: 2.222074508666992\n",
      "[step: 378] loss: 2.223301649093628\n",
      "[step: 379] loss: 2.221190929412842\n",
      "[step: 380] loss: 2.220301866531372\n",
      "[step: 381] loss: 2.220299005508423\n",
      "[step: 382] loss: 2.219499349594116\n",
      "[step: 383] loss: 2.218672752380371\n",
      "[step: 384] loss: 2.2181997299194336\n",
      "[step: 385] loss: 2.218031167984009\n",
      "[step: 386] loss: 2.2177674770355225\n",
      "[step: 387] loss: 2.2168166637420654\n",
      "[step: 388] loss: 2.216282367706299\n",
      "[step: 389] loss: 2.2159831523895264\n",
      "[step: 390] loss: 2.2153892517089844\n",
      "[step: 391] loss: 2.214893102645874\n",
      "[step: 392] loss: 2.214430332183838\n",
      "[step: 393] loss: 2.2142863273620605\n",
      "[step: 394] loss: 2.2142226696014404\n",
      "[step: 395] loss: 2.2134406566619873\n",
      "[step: 396] loss: 2.212984323501587\n",
      "[step: 397] loss: 2.212036609649658\n",
      "[step: 398] loss: 2.2116825580596924\n",
      "[step: 399] loss: 2.2112607955932617\n",
      "[step: 400] loss: 2.210958957672119\n",
      "[step: 401] loss: 2.2105422019958496\n",
      "[step: 402] loss: 2.2101423740386963\n",
      "[step: 403] loss: 2.2094085216522217\n",
      "[step: 404] loss: 2.209212303161621\n",
      "[step: 405] loss: 2.2096900939941406\n",
      "[step: 406] loss: 2.211106300354004\n",
      "[step: 407] loss: 2.2089850902557373\n",
      "[step: 408] loss: 2.2080259323120117\n",
      "[step: 409] loss: 2.207598924636841\n",
      "[step: 410] loss: 2.2065231800079346\n",
      "[step: 411] loss: 2.2077417373657227\n",
      "[step: 412] loss: 2.205928087234497\n",
      "[step: 413] loss: 2.2058515548706055\n",
      "[step: 414] loss: 2.2044870853424072\n",
      "[step: 415] loss: 2.2046244144439697\n",
      "[step: 416] loss: 2.20365571975708\n",
      "[step: 417] loss: 2.203403949737549\n",
      "[step: 418] loss: 2.2027485370635986\n",
      "[step: 419] loss: 2.2028825283050537\n",
      "[step: 420] loss: 2.202808380126953\n",
      "[step: 421] loss: 2.202303409576416\n",
      "[step: 422] loss: 2.2015671730041504\n",
      "[step: 423] loss: 2.2010555267333984\n",
      "[step: 424] loss: 2.2003142833709717\n",
      "[step: 425] loss: 2.1997811794281006\n",
      "[step: 426] loss: 2.1992971897125244\n",
      "[step: 427] loss: 2.1988894939422607\n",
      "[step: 428] loss: 2.19844126701355\n",
      "[step: 429] loss: 2.198094367980957\n",
      "[step: 430] loss: 2.1976053714752197\n",
      "[step: 431] loss: 2.197340965270996\n",
      "[step: 432] loss: 2.1966896057128906\n",
      "[step: 433] loss: 2.1963772773742676\n",
      "[step: 434] loss: 2.1961328983306885\n",
      "[step: 435] loss: 2.196790933609009\n",
      "[step: 436] loss: 2.1970083713531494\n",
      "[step: 437] loss: 2.2010035514831543\n",
      "[step: 438] loss: 2.19995379447937\n",
      "[step: 439] loss: 2.201367139816284\n",
      "[step: 440] loss: 2.1938564777374268\n",
      "[step: 441] loss: 2.195026397705078\n",
      "[step: 442] loss: 2.197786569595337\n",
      "[step: 443] loss: 2.1948537826538086\n",
      "[step: 444] loss: 2.1919608116149902\n",
      "[step: 445] loss: 2.192547559738159\n",
      "[step: 446] loss: 2.1917762756347656\n",
      "[step: 447] loss: 2.191716194152832\n",
      "[step: 448] loss: 2.1903316974639893\n",
      "[step: 449] loss: 2.1907780170440674\n",
      "[step: 450] loss: 2.1900763511657715\n",
      "[step: 451] loss: 2.1906073093414307\n",
      "[step: 452] loss: 2.189039945602417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 453] loss: 2.189129590988159\n",
      "[step: 454] loss: 2.1880369186401367\n",
      "[step: 455] loss: 2.1895318031311035\n",
      "[step: 456] loss: 2.189283847808838\n",
      "[step: 457] loss: 2.1901369094848633\n",
      "[step: 458] loss: 2.187340021133423\n",
      "[step: 459] loss: 2.1870651245117188\n",
      "[step: 460] loss: 2.186671495437622\n",
      "[step: 461] loss: 2.1867425441741943\n",
      "[step: 462] loss: 2.1855525970458984\n",
      "[step: 463] loss: 2.185385227203369\n",
      "[step: 464] loss: 2.184576988220215\n",
      "[step: 465] loss: 2.18448805809021\n",
      "[step: 466] loss: 2.1835641860961914\n",
      "[step: 467] loss: 2.1841177940368652\n",
      "[step: 468] loss: 2.1835196018218994\n",
      "[step: 469] loss: 2.1838037967681885\n",
      "[step: 470] loss: 2.183342456817627\n",
      "[step: 471] loss: 2.184002161026001\n",
      "[step: 472] loss: 2.1825547218322754\n",
      "[step: 473] loss: 2.1819710731506348\n",
      "[step: 474] loss: 2.1809253692626953\n",
      "[step: 475] loss: 2.1801741123199463\n",
      "[step: 476] loss: 2.1799817085266113\n",
      "[step: 477] loss: 2.180187702178955\n",
      "[step: 478] loss: 2.1804518699645996\n",
      "[step: 479] loss: 2.180575370788574\n",
      "[step: 480] loss: 2.1813673973083496\n",
      "[step: 481] loss: 2.180910110473633\n",
      "[step: 482] loss: 2.179877996444702\n",
      "[step: 483] loss: 2.178004026412964\n",
      "[step: 484] loss: 2.177105188369751\n",
      "[step: 485] loss: 2.176774263381958\n",
      "[step: 486] loss: 2.176212787628174\n",
      "[step: 487] loss: 2.1758975982666016\n",
      "[step: 488] loss: 2.175682306289673\n",
      "[step: 489] loss: 2.1754112243652344\n",
      "[step: 490] loss: 2.1755058765411377\n",
      "[step: 491] loss: 2.174675226211548\n",
      "[step: 492] loss: 2.175271987915039\n",
      "[step: 493] loss: 2.1739144325256348\n",
      "[step: 494] loss: 2.1736910343170166\n",
      "[step: 495] loss: 2.1735424995422363\n",
      "[step: 496] loss: 2.1733720302581787\n",
      "[step: 497] loss: 2.1740331649780273\n",
      "[step: 498] loss: 2.1763978004455566\n",
      "[step: 499] loss: 2.1792452335357666\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _, step_loss = final_sess.run([train, loss], feed_dict={\n",
    "                            X: finalX, Y: finalY})\n",
    "    print(\"[step: {}] loss: {}\".format(i, step_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divide_size :  15000\n",
      "X_test shape :  (26240, 13)\n",
      "X_test_final shape :  (2624, 10, 13)\n",
      "predict shape :  (2624, 1)\n"
     ]
    }
   ],
   "source": [
    "#s_t = time.time()\n",
    "\n",
    "test_path = 'test'\n",
    "test_list = os.listdir(test_path)\n",
    "X_test = []\n",
    "number_of_csvs = 2624\n",
    "number_of_rows_in_each_csvs = 150000\n",
    "divide_size = int(number_of_rows_in_each_csvs/seq_length)\n",
    "print(\"divide_size : \", divide_size)\n",
    "\n",
    "\n",
    "for path in test_list:\n",
    "    test = pd.read_csv(os.path.join(test_path, path), dtype=np.float64)\n",
    "    #print(\"test shape : \", test.shape)\n",
    "    for i in range(0, seq_length):\n",
    "        #test_divided_by_size = test[i*size:(i+1)*15000, :]\n",
    "        test_divided_by_length = test[i*divide_size:(i+1)*divide_size]\n",
    "        #print(\"test_divided_by_length shape : \", test_divided_by_length.shape)\n",
    "\n",
    "        X_test.append(gen_features(test_divided_by_length).values)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "print(\"X_test shape : \", X_test.shape) #maybe 2624*10 = 26240 (26240,13)\n",
    "#print(X_test[0:1000, :])\n",
    "\n",
    "def build_dataset_for_predict(XX, seq_length):\n",
    "    dataX = []\n",
    "    for i in range(0, number_of_csvs*seq_length,seq_length): # 0 ~ 26230\n",
    "        _x = XX[i:i + seq_length, :]\n",
    "        dataX.append(_x)\n",
    "    return np.array(dataX)\n",
    "\n",
    "X_test_final = build_dataset_for_predict(X_test, seq_length)\n",
    "print(\"X_test_final shape : \", X_test_final.shape) # (26240, 10, 13)\n",
    "\n",
    "#X_test = MinMaxScaler(X_test)\n",
    "#scaler = StandardScaler().fit(X_test)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "predict = final_sess.run(Y_pred, feed_dict={X: X_test_final})\n",
    "print(\"predict shape : \", predict.shape)\n",
    "\n",
    "# reg = reg_grid.best_estimator_\n",
    "# reg.fit(X_train, y_train)\n",
    "# y_pred = reg.predict(X_test)\n",
    "pd.DataFrame(predict).to_csv('20190322_prediction_LSTM_reduced_mean_0.005_final_sess.csv', header=None)\n",
    "\n",
    "#e_t = time.time()\n",
    "#duration = (e_t - s_t) / 60\n",
    "#print(\"{:.1f} min tooked\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
